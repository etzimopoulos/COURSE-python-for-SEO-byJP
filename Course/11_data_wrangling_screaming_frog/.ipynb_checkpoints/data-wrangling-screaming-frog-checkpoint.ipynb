{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Data Wrangling With Screaming Frog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Outcomes\n",
    "\n",
    "- To learn how to automate the command line Screaming Frog commands with Python\n",
    "- To learn how to wrangle 5 .csv files from Screaming Frog with Pandas\n",
    "- To learn how to push the data into a BigQuery table\n",
    "- To learn how to connect your BigQuery table to Google Data Studio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last tutorial, you learned how to [easily automate Screaming Frog on the command line for either Mac or Windows.](https://sempioneer.com/python-for-seo/screaming-frog-automation/).\n",
    "\n",
    "In this section we'll be focusing on automating the previous terminal commands with Python.\n",
    "\n",
    "Then we'll wrangle the .csv data into Pandas, push it into [BigQuery](https://cloud.google.com/bigquery) and finally view it in [Google Data Studio.](https://datastudio.google.com/u/0/navigation/reporting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "    \n",
    "- https://docs.python.org/3/library/subprocess.html\n",
    "- https://www.jstorimer.com/blogs/workingwithcode/7766119-when-to-use-stderr-instead-of-stdout\n",
    "- https://pandas.pydata.org/\n",
    "- https://www.vervesearch.com/blog/screaming-frog-google-compute-cloud-automatically-crawl-an-entire-industry-fast/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Scripts To Refactor:</strong>\n",
    "- https://www.vervesearch.com/screaming-frog-files/scream.py\n",
    "- https://www.vervesearch.com/screaming-frog-files/auto-ssh.py\n",
    "- https://raw.githubusercontent.com/skywind3000/terminal/master/terminal.py\n",
    "- https://www.vervesearch.com/blog/compare-screaming-frog-crawl-files/\n",
    "- https://github.com/skywind3000/terminal/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.7/site-packages (1.0.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /Users/jamesaphoenix/.local/lib/python3.7/site-packages (from pandas) (2.8.0)\r\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/anaconda3/lib/python3.7/site-packages (from pandas) (2019.3)\r\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/anaconda3/lib/python3.7/site-packages (from pandas) (1.18.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/jamesaphoenix/.local/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas) (1.11.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'collections_abc' from 'six.moves' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-230-2c4855e2c65d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Google Libraries:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moauth2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mservice_account\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbigquery\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"google-cloud-bigquery\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbigquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbigquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAccessEntry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbigquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/google/cloud/bigquery/client.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbigquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_helpers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_verify_job_config_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbigquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_http\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbigquery\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pandas_helpers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbigquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbigquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDatasetListItem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/google/cloud/bigquery/_pandas_helpers.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mpyarrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbigquery\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/google/cloud/bigquery/schema.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\"\"\"Schemas for BigQuery tables / queries.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollections_abc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbigquery_v2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'collections_abc' from 'six.moves' (unknown location)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "from sys import platform\n",
    "\n",
    "# Google Libraries:\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How To Run The Command Line In Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we'll be using linux commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletedProcess(args='ls', returncode=0, stdout=b'data-wrangling-screaming-frog.ipynb\\n', stderr=b'')\n"
     ]
    }
   ],
   "source": [
    "process = subprocess.run(\"ls\", shell=True, check=True, capture_output=True)\n",
    "print(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the return code of the subprocess: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"This is the return code of the subprocess: {process.returncode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Typically a <strong> returncode 0 means that the command run successfully. </strong>\n",
    "- Also notice how the output of the command is pushed into stdout (standard output), stderr (standard errror)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How To Run The Screaming Frog Command Line Scripts With Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract our username, website, output location and Screaming Frog application and put them into variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jamesaphoenix/Desktop/Imran_And_James/Python_For_SEO/11_data_wrangling_screaming_frog\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = 'jamesaphoneix'\n",
    "website = 'https://phoenixandpartners.co.uk/'\n",
    "output_location = '/users/jamesaphoenix/desktop'\n",
    "screaming_frog_app = '/Applications/Screaming\\ Frog\\ SEO\\ Spider.app/Contents/MacOS/ScreamingFrogSEOSpiderLauncher'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Username: jamesaphoneix '\n",
      "Website: https://phoenixandpartners.co.uk/ \n",
      "OutputLocation: --output-folder /users/jamesaphoneix/desktop \n",
      "ScreamingFrogLocation: /Applications/Screaming\\ Frog\\ SEO\\ Spider.app/Contents/MacOS/ScreamingFrogSEOSpiderLauncher\n"
     ]
    }
   ],
   "source": [
    "print(\"Username:\",username, \"'\\nWebsite:\",website, '\\nOutputLocation:', output_location,\n",
    "     \"\\nScreamingFrogLocation:\",screaming_frog_app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a couple of Screaming Frog string commands that we'll push into subprocess commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "screaming_frog_open = \"/Applications/Screaming Frog SEO Spider.app/Contents/MacOS/ScreamingFrogSEOSpiderLauncher\"\n",
    "screaming_frog_crawl=f'{screaming_frog_app} --headless --save-crawl --output-folder {output_location} --timestamped-output --crawl phoenixandpartners.co.uk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Applications/Screaming\\\\ Frog\\\\ SEO\\\\ Spider.app/Contents/MacOS/ScreamingFrogSEOSpiderLauncher --headless --save-crawl --output-folder /users/jamesaphoenix/desktop --timestamped-output --crawl phoenixandpartners.co.uk'"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "screaming_frog_crawl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also notice, how we've used an f string for the screaming_frog_crawl variable, which means:\n",
    "\n",
    "- jamesaphoenix will be passed into this text string instead of {username}.\n",
    "- https://phoenixandpartners.co.uk/ will be passed into this text string instead of {website}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Applications/Screaming\\ Frog\\ SEO\\ Spider.app/Contents/MacOS/ScreamingFrogSEOSpiderLauncher --headless --save-crawl --output-folder /users/jamesaphoenix/desktop --timestamped-output --crawl phoenixandpartners.co.uk\n"
     ]
    }
   ],
   "source": [
    "print(screaming_frog_crawl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now run them one by one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_sf = subprocess.run(screaming_frog_open)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command will hopefully open scremaing frog, also the subprocess will keep running until we close the window.\n",
    "\n",
    "So close screaming frog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletedProcess(args='/Applications/Screaming Frog SEO Spider.app/Contents/MacOS/ScreamingFrogSEOSpiderLauncher', returncode=0)\n"
     ]
    }
   ],
   "source": [
    "print(open_sf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the return code was 0!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "screaming_frog=subprocess.run(screaming_frog_crawl, \n",
    "               shell=True, \n",
    "               capture_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Applications/Screaming\\\\ Frog\\\\ SEO\\\\ Spider.app/Contents/MacOS/ScreamingFrogSEOSpiderLauncher --headless --save-crawl --output-folder /users/jamesaphoenix/desktop --timestamped-output --crawl phoenixandpartners.co.uk'"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "screaming_frog_crawl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://sempioneer.com/wp-content/uploads/2020/06/screaming-frog-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How To Find The Outputted Folder Name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as saving the crawl, we can parse the standard output pipe (stdout) and obtain the name of the timestamped folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'args',\n",
       " 'check_returncode',\n",
       " 'returncode',\n",
       " 'stderr',\n",
       " 'stdout']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(screaming_frog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can decode the stdout which will convert all of the console messages into a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-25\n"
     ]
    }
   ],
   "source": [
    "text = screaming_frog.stdout.decode('utf-8')\n",
    "print(text[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's search the date timestamp:\n",
    "\n",
    "- after this: <strong> Output directory: </strong>\n",
    "- before this: <strong> \\n </strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://sempioneer.com/wp-content/uploads/2020/06/output-directory.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the timestamped output folder: /users/jamesaphoenix/desktop/2020.06.25.15.01.49\n"
     ]
    }
   ],
   "source": [
    "timestamp = re.findall('(?<=Output directory:)(.*?)(?=\\n)', \n",
    "                       str(text))\n",
    "\n",
    "correct_folder = timestamp[0].strip()\n",
    "print(f\"This is the timestamped output folder: {correct_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check what folders are in our current working directory with:\n",
    "\n",
    "~~~\n",
    "\n",
    "os.listdir()\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Music',\n",
       " 'Screaming Frog - Data Manipulation.ipynb',\n",
       " 'ngrok',\n",
       " '2020.06.25.15.01.49',\n",
       " '.DS_Store',\n",
       " '.localized',\n",
       " 'config',\n",
       " 'Coding_Marketing_Projects',\n",
       " 'Google_cloud-sdk',\n",
       " 'screaming-frog-remotedesktop-image.vmdk',\n",
       " 'Screenshot 2020-06-25 at 15.07.17.png',\n",
       " 'Extracting Schema At Bulk.ipynb',\n",
       " 'Scripts_and_keys',\n",
       " 'YouTube SEO.jpg',\n",
       " 'Data_Science_Resources',\n",
       " 'Screenshot 2020-06-25 at 15.07.17 (2).png',\n",
       " 'Marketing',\n",
       " 'Sort Through These',\n",
       " 'Atom.app',\n",
       " 'Math Textbooks',\n",
       " '.ipynb_checkpoints',\n",
       " 'Client_Projects',\n",
       " 'Imran_And_James',\n",
       " 'General_Assembly',\n",
       " 'layered_architecture.png',\n",
       " 'Postman.app',\n",
       " 'message.png']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('/Users/jamesaphoenix/Desktop') # This changes the directory into the desktop\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://sempioneer.com/wp-content/uploads/2020/06/output-of-directory.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to capture the relevant folder would be to:\n",
    "    \n",
    "1. Get todays date.\n",
    "2. Only return folders that include todays date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020.06.25\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "todays_date = now.strftime(\"%Y.%m.%d\")\n",
    "print(todays_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2020.06.25.15.01.49']\n"
     ]
    }
   ],
   "source": [
    "screaming_frog_folders = [file for file in os.listdir() if todays_date in file]\n",
    "print(screaming_frog_folders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhancing Our Screaming Frog CLI Automation With Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the subprocess and string command is a better improvement then having to load up terminal and manually enter in the commands.\n",
    "\n",
    "But let's take it a step further. I've created several classes such as 🐸 <strong> ScreamingFrogAnalyser, CSVParser and BigQueryAutomation</strong> 🐸\n",
    "\n",
    "These will allow you to:\n",
    "\n",
    "- Run a certain number of website crawls.\n",
    "- All of the folders/files will be outputted and collected.\n",
    "- All of the CSV data will be merged across multiple domains and the domain name added as an extra column.\n",
    "\n",
    "----\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find all of these scripts inside of the src folder of this module. \n",
    "\n",
    "If you would like to dive deeper into the code you can view it:\n",
    "- Here\n",
    "- Here\n",
    "- Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How To Setup Executecrawl.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However if you'd just like to make the most of it, I've created a function wrapper for it called execute_crawl.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this python file, you can find a function called run:\n",
    "    \n",
    "You will need to first gather all of the relevant information and populate it below so that the script can run correctly:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <strong> GOOGLE_CLOUD_PROJECT_ID: </strong> This is your Google Cloud Project ID \n",
    "\n",
    "- <strong> OUTPUTFOLDER: </strong> This is your desired output folder for the screaming frog crawls. For example my outputfolder is: <strong> /Users/jamesaphoenix/Desktop </strong>\n",
    "-  <strong> SERVICE_ACCOUNT_KEY_LOCATION: </strong>  You will need a service account key for us to automatically create and upload to BigQuery tables.\n",
    "-  <strong> website_urls: </strong>  You need to provide a python list of website URL's that you'd like to crawl such as:\n",
    "\n",
    "~~~\n",
    "['https://phoenixandpartners.co.uk/', 'https://sempioneer.com/']\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BigQuery Setup\n",
    "\n",
    "- API Creation\n",
    "- Table Creation\n",
    "- Service Account Key Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TBC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pushing The Data To BigQuery\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TBC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting The BigQuery Table to Google Data Studio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
