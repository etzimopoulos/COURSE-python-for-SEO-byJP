{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. How To Download Multiple Images In Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Outcomes\n",
    "\n",
    "- To learn how to download multiple images in Python using synchronous and asynchronous code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatically downloading images from a number of your HTML pages is an essential skill, in this guide you'll be learning 4 methods on how to download images using Python! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin with the easiest example, if you already have a list of image URLs then we can follow this process:\n",
    "\n",
    "1. Change into a directory where we would like to store all of the images.\n",
    "2. Make a request to download all of the images, one by one.\n",
    "3. We will also include error handling so that if a URL no longer exists the code will still work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tldextract in /opt/anaconda3/lib/python3.7/site-packages (2.2.2)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.7/site-packages (from tldextract) (46.0.0.post20200309)\n",
      "Requirement already satisfied: requests>=2.1.0 in /opt/anaconda3/lib/python3.7/site-packages (from tldextract) (2.22.0)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.7/site-packages (from tldextract) (2.8)\n",
      "Requirement already satisfied: requests-file>=1.4 in /opt/anaconda3/lib/python3.7/site-packages (from tldextract) (1.5.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda3/lib/python3.7/site-packages (from requests>=2.1.0->tldextract) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.7/site-packages (from requests>=2.1.0->tldextract) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.7/site-packages (from requests>=2.1.0->tldextract) (3.0.4)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.7/site-packages (from requests-file>=1.4->tldextract) (1.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tldextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import subprocess\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import tldextract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir all_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mall_images\u001b[m\u001b[m\r\n",
      "asyncio-aiofiles.py\r\n",
      "how-to-download-multiple-images-starter-code.ipynb\r\n",
      "how-to-download-multiple-images.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing into the directory of the folder called all_images, this can be done by either:\n",
    "\n",
    "~~~\n",
    "\n",
    "cd all_images\n",
    "os.chdir('path')\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('all_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jamesaphoenix/Desktop/Imran_And_James/Python_For_SEO/6_downloading_multiple_images/all_images\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method One: How To Download Multiple Images From A Python List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to download the multiple images, we can use the [requests library](https://requests.readthedocs.io/en/master/). We'll also create a python list to store any images that didn't have a 200 status code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "broken_images = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_urls = ['https://sempioneer.com/wp-content/uploads/2020/05/dataframe-300x84.png',\n",
    "             'https://sempioneer.com/wp-content/uploads/2020/05/json_format_data-300x72.png']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframe-300x84.png\n",
      "https://sempioneer.com/wp-content/uploads/2020/05/dataframe-300x84.png\n",
      "json_format_data-300x72.png\n",
      "https://sempioneer.com/wp-content/uploads/2020/05/json_format_data-300x72.png\n"
     ]
    }
   ],
   "source": [
    "for img in image_urls:\n",
    "    \n",
    "    file_name = img.split('/')[-1]\n",
    "    \n",
    "    r = requests.get(img, stream=True)\n",
    "    \n",
    "    if r.status_code == 200:\n",
    "        with open(file_name, 'wb') as f:\n",
    "            for chunk in r:\n",
    "                f.write(chunk)\n",
    "    else:\n",
    "        broken_images.append(img)\n",
    "    \n",
    "    print(file_name)\n",
    "    \n",
    "    print(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "☝️ See how simple that is! ☝️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you check your folder, you will have now downloaded all of the images that contained a status code of 200! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![downloading images correctly with python](https://sempioneer.com/wp-content/uploads/2020/06/how-to-download-images-with-python.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method Two: How To Download Multiple Images From Many HTML Web Pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we don't yet have the exact image URLs, we will need to do the following:\n",
    "\n",
    "1. Download the HTML content of every web page.\n",
    "2. Extract all of the image URLs for every page.\n",
    "3. Create the file names.\n",
    "4. Check to see if the image status code is 200.\n",
    "5. Write all of images to your local computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This website [internetingishard.com](https://www.internetingishard.com/html-and-css/links-and-images/) has some relative image URLs. Therefore we will need to ensure that our code can handle for the following two types of image source URLs:\n",
    "\n",
    "---\n",
    "\n",
    "- <strong> Exact Filepath: https://www.internetingishard.com/html-and-css/links-and-images/html-attributes-6f5690.png </strong>\n",
    "- <strong> Relative Filepath: /html-and-css/links-and-images/html-attributes-6f5690.png </strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_pages = ['https://understandingdata.com/', \n",
    "             'https://understandingdata.com/data-engineering-services/',\n",
    "             'https://www.internetingishard.com/html-and-css/links-and-images/']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also extract the domain of every URL whilst we loop over the webpages like so:\n",
    "    \n",
    "~~~\n",
    "\n",
    "for page in web_pages:\n",
    "    domain_name = tldextract.extract(page).registered_domain\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_dictionary = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "understandingdata.com\n",
      "understandingdata.com\n",
      "internetingishard.com\n"
     ]
    }
   ],
   "source": [
    "for page in web_pages:\n",
    "    # 1. Extract domain name\n",
    "    domain_name = tldextract.extract(page).registered_domain\n",
    "    print(domain_name)\n",
    "    \n",
    "    r = requests.get(page)\n",
    "    \n",
    "    if r.status_code == 200:\n",
    "        \n",
    "        url_dictionary[page] = []\n",
    "        \n",
    "        soup = BeautifulSoup(r.content, 'html.parser')\n",
    "        \n",
    "        images = soup.findAll('img')\n",
    "        \n",
    "        url_dictionary[page].extend(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's double check and filter our dictionary so that we only look at web pages where there was at least 1 image tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<img alt=\"Just Understanding Data\" height=\"136\" src=\"//understandingdata.com/wp-content/uploads/2019/04/cropped-logo_transparent-1.png\" width=\"1200\"/>, <img alt=\"Just Understanding Data\" height=\"136\" src=\"//understandingdata.com/wp-content/uploads/2019/04/cropped-logo_transparent-1.png\" width=\"1200\"/>, <img src=\"https://understandingdata.com/wp-content/uploads/2019/09/james-anthony-phoenix.jpg\" style=\"border-radius: 50%; max-width:75%; padding-bottom:15px;\"/>, <img class=\"desktop-image\" src=\"https://understandingdata.com/wp-content/uploads/2019/09/james-anthony-phoenix.jpg\" style=\"border-radius: 50%; max-width:390px;\"/>, <img alt=\"is web scraping illegal? header image\" class=\"attachment-gutentype-thumb-masonry size-gutentype-thumb-masonry wp-post-image\" height=\"370\" sizes=\"(max-width: 370px) 100vw, 370px\" src=\"https://understandingdata.com/wp-content/uploads/2020/05/Is-Web-Scraping-Illegal-370x370.png\" srcset=\"https://understandingdata.com/wp-content/uploads/2020/05/Is-Web-Scraping-Illegal-370x370.png 370w, https://understandingdata.com/wp-content/uploads/2020/05/Is-Web-Scraping-Illegal-300x300.png 300w, https://understandingdata.com/wp-content/uploads/2020/05/Is-Web-Scraping-Illegal-1024x1024.png 1024w, https://understandingdata.com/wp-content/uploads/2020/05/Is-Web-Scraping-Illegal-150x150.png 150w, https://understandingdata.com/wp-content/uploads/2020/05/Is-Web-Scraping-Illegal-768x768.png 768w, https://understandingdata.com/wp-content/uploads/2020/05/Is-Web-Scraping-Illegal-90x90.png 90w, https://understandingdata.com/wp-content/uploads/2020/05/Is-Web-Scraping-Illegal-180x180.png 180w, https://understandingdata.com/wp-content/uploads/2020/05/Is-Web-Scraping-Illegal-760x760.png 760w, https://understandingdata.com/wp-content/uploads/2020/05/Is-Web-Scraping-Illegal-740x740.png 740w, https://understandingdata.com/wp-content/uploads/2020/05/Is-Web-Scraping-Illegal.png 1080w\" width=\"370\"><div class=\"mask\"></div> <a aria-hidden=\"true\" class=\"icons\" href=\"https://understandingdata.com/is-web-scraping-legal/\"></a></img>, <img alt=\"\" class=\"avatar avatar-35 photo\" height=\"35\" src=\"https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&amp;r=g\" srcset=\"https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=70&amp;r=g 2x\" width=\"35\"> </img>, <img alt=\"web scraping tools\" class=\"attachment-gutentype-thumb-masonry size-gutentype-thumb-masonry wp-post-image\" height=\"192\" sizes=\"(max-width: 370px) 100vw, 370px\" src=\"https://understandingdata.com/wp-content/uploads/2020/03/web-scraping-tools-370x192.jpg\" srcset=\"https://understandingdata.com/wp-content/uploads/2020/03/web-scraping-tools-370x192.jpg 370w, https://understandingdata.com/wp-content/uploads/2020/03/web-scraping-tools-300x155.jpg 300w, https://understandingdata.com/wp-content/uploads/2020/03/web-scraping-tools-1024x530.jpg 1024w, https://understandingdata.com/wp-content/uploads/2020/03/web-scraping-tools-768x398.jpg 768w, https://understandingdata.com/wp-content/uploads/2020/03/web-scraping-tools-760x394.jpg 760w, https://understandingdata.com/wp-content/uploads/2020/03/web-scraping-tools-740x383.jpg 740w, https://understandingdata.com/wp-content/uploads/2020/03/web-scraping-tools.jpg 1197w\" width=\"370\"><div class=\"mask\"></div> <a aria-hidden=\"true\" class=\"icons\" href=\"https://understandingdata.com/web-scraping-tools/\"></a></img>, <img alt=\"\" class=\"avatar avatar-35 photo\" height=\"35\" src=\"https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&amp;r=g\" srcset=\"https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=70&amp;r=g 2x\" width=\"35\"> </img>, <img alt=\"community detection graph example with four groups.\" class=\"attachment-gutentype-thumb-masonry size-gutentype-thumb-masonry wp-post-image\" height=\"238\" sizes=\"(max-width: 370px) 100vw, 370px\" src=\"https://understandingdata.com/wp-content/uploads/2020/03/community-detection-370x238.png\" srcset=\"https://understandingdata.com/wp-content/uploads/2020/03/community-detection-370x238.png 370w, https://understandingdata.com/wp-content/uploads/2020/03/community-detection-300x193.png 300w, https://understandingdata.com/wp-content/uploads/2020/03/community-detection.png 700w\" width=\"370\"/>, <img alt=\"\" class=\"avatar avatar-35 photo\" height=\"35\" src=\"https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&amp;r=g\" srcset=\"https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=70&amp;r=g 2x\" width=\"35\"/>, <img alt=\"what is web scraping\" class=\"attachment-gutentype-thumb-masonry size-gutentype-thumb-masonry wp-post-image\" height=\"370\" sizes=\"(max-width: 370px) 100vw, 370px\" src=\"https://understandingdata.com/wp-content/uploads/2020/03/what-is-web-scraping-370x370.png\" srcset=\"https://understandingdata.com/wp-content/uploads/2020/03/what-is-web-scraping-370x370.png 370w, https://understandingdata.com/wp-content/uploads/2020/03/what-is-web-scraping-300x300.png 300w, https://understandingdata.com/wp-content/uploads/2020/03/what-is-web-scraping-1024x1024.png 1024w, https://understandingdata.com/wp-content/uploads/2020/03/what-is-web-scraping-150x150.png 150w, https://understandingdata.com/wp-content/uploads/2020/03/what-is-web-scraping-768x768.png 768w, https://understandingdata.com/wp-content/uploads/2020/03/what-is-web-scraping-90x90.png 90w, https://understandingdata.com/wp-content/uploads/2020/03/what-is-web-scraping-180x180.png 180w, https://understandingdata.com/wp-content/uploads/2020/03/what-is-web-scraping-760x760.png 760w, https://understandingdata.com/wp-content/uploads/2020/03/what-is-web-scraping-740x740.png 740w, https://understandingdata.com/wp-content/uploads/2020/03/what-is-web-scraping.png 1080w\" width=\"370\"/>, <img alt=\"\" class=\"avatar avatar-35 photo\" height=\"35\" src=\"https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&amp;r=g\" srcset=\"https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=70&amp;r=g 2x\" width=\"35\"/>, <img alt=\"installing chromedriver and google chrome on AWS\" class=\"attachment-gutentype-thumb-masonry size-gutentype-thumb-masonry wp-post-image\" height=\"300\" sizes=\"(max-width: 300px) 100vw, 300px\" src=\"https://understandingdata.com/wp-content/uploads/2020/03/installing-chromedriver-headless-browser.png\" srcset=\"https://understandingdata.com/wp-content/uploads/2020/03/installing-chromedriver-headless-browser.png 300w, https://understandingdata.com/wp-content/uploads/2020/03/installing-chromedriver-headless-browser-150x150.png 150w, https://understandingdata.com/wp-content/uploads/2020/03/installing-chromedriver-headless-browser-90x90.png 90w, https://understandingdata.com/wp-content/uploads/2020/03/installing-chromedriver-headless-browser-180x180.png 180w\" width=\"300\"/>, <img alt=\"\" class=\"avatar avatar-35 photo\" height=\"35\" src=\"https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&amp;r=g\" srcset=\"https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=70&amp;r=g 2x\" width=\"35\"/>, <img alt=\"how to install anaconda on Mac OS X\" class=\"attachment-gutentype-thumb-masonry size-gutentype-thumb-masonry wp-post-image\" height=\"208\" sizes=\"(max-width: 370px) 100vw, 370px\" src=\"https://understandingdata.com/wp-content/uploads/2020/02/how-to-install-python-anaconda-mac-370x208.png\" srcset=\"https://understandingdata.com/wp-content/uploads/2020/02/how-to-install-python-anaconda-mac-370x208.png 370w, https://understandingdata.com/wp-content/uploads/2020/02/how-to-install-python-anaconda-mac-300x169.png 300w, https://understandingdata.com/wp-content/uploads/2020/02/how-to-install-python-anaconda-mac-1024x576.png 1024w, https://understandingdata.com/wp-content/uploads/2020/02/how-to-install-python-anaconda-mac-768x432.png 768w, https://understandingdata.com/wp-content/uploads/2020/02/how-to-install-python-anaconda-mac-1170x658.png 1170w, https://understandingdata.com/wp-content/uploads/2020/02/how-to-install-python-anaconda-mac-972x546.png 972w, https://understandingdata.com/wp-content/uploads/2020/02/how-to-install-python-anaconda-mac-740x416.png 740w, https://understandingdata.com/wp-content/uploads/2020/02/how-to-install-python-anaconda-mac-760x428.png 760w, https://understandingdata.com/wp-content/uploads/2020/02/how-to-install-python-anaconda-mac-270x152.png 270w, https://understandingdata.com/wp-content/uploads/2020/02/how-to-install-python-anaconda-mac-540x304.png 540w, https://understandingdata.com/wp-content/uploads/2020/02/how-to-install-python-anaconda-mac.png 1280w\" width=\"370\"/>, <img alt=\"\" class=\"avatar avatar-35 photo\" height=\"35\" src=\"https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&amp;r=g\" srcset=\"https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=70&amp;r=g 2x\" width=\"35\"/>, <img alt=\"web scraping ninja outreach\" class=\"attachment-gutentype-thumb-masonry size-gutentype-thumb-masonry wp-post-image\" height=\"194\" sizes=\"(max-width: 370px) 100vw, 370px\" src=\"https://understandingdata.com/wp-content/uploads/2019/12/ninjaoutreach-black-friday-deal-370x194.png\" srcset=\"https://understandingdata.com/wp-content/uploads/2019/12/ninjaoutreach-black-friday-deal-370x194.png 370w, https://understandingdata.com/wp-content/uploads/2019/12/ninjaoutreach-black-friday-deal-300x157.png 300w, https://understandingdata.com/wp-content/uploads/2019/12/ninjaoutreach-black-friday-deal-768x402.png 768w, https://understandingdata.com/wp-content/uploads/2019/12/ninjaoutreach-black-friday-deal-760x398.png 760w, https://understandingdata.com/wp-content/uploads/2019/12/ninjaoutreach-black-friday-deal-740x388.png 740w, https://understandingdata.com/wp-content/uploads/2019/12/ninjaoutreach-black-friday-deal.png 800w\" width=\"370\"/>, <img alt=\"\" class=\"avatar avatar-35 photo\" height=\"35\" src=\"https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&amp;r=g\" srcset=\"https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=70&amp;r=g 2x\" width=\"35\"/>, <img alt=\"web scraping advantages and disadvantages\" class=\"attachment-gutentype-thumb-masonry size-gutentype-thumb-masonry wp-post-image\" height=\"370\" sizes=\"(max-width: 370px) 100vw, 370px\" src=\"https://understandingdata.com/wp-content/uploads/2020/02/webscraping_advantages_disadvantages-370x370.png\" srcset=\"https://understandingdata.com/wp-content/uploads/2020/02/webscraping_advantages_disadvantages-370x370.png 370w, https://understandingdata.com/wp-content/uploads/2020/02/webscraping_advantages_disadvantages-300x300.png 300w, https://understandingdata.com/wp-content/uploads/2020/02/webscraping_advantages_disadvantages-1024x1024.png 1024w, https://understandingdata.com/wp-content/uploads/2020/02/webscraping_advantages_disadvantages-150x150.png 150w, https://understandingdata.com/wp-content/uploads/2020/02/webscraping_advantages_disadvantages-768x768.png 768w, https://understandingdata.com/wp-content/uploads/2020/02/webscraping_advantages_disadvantages-90x90.png 90w, https://understandingdata.com/wp-content/uploads/2020/02/webscraping_advantages_disadvantages-180x180.png 180w, https://understandingdata.com/wp-content/uploads/2020/02/webscraping_advantages_disadvantages-760x760.png 760w, https://understandingdata.com/wp-content/uploads/2020/02/webscraping_advantages_disadvantages-740x740.png 740w, https://understandingdata.com/wp-content/uploads/2020/02/webscraping_advantages_disadvantages.png 1080w\" width=\"370\"/>, <img alt=\"\" class=\"avatar avatar-35 photo\" height=\"35\" src=\"https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&amp;r=g\" srcset=\"https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=70&amp;r=g 2x\" width=\"35\"/>, <img alt=\"prospecting seo clients with google\" class=\"attachment-gutentype-thumb-masonry size-gutentype-thumb-masonry wp-post-image\" height=\"370\" sizes=\"(max-width: 370px) 100vw, 370px\" src=\"https://understandingdata.com/wp-content/uploads/2019/11/prospecting-seo-clients-with-python-370x370.png\" srcset=\"https://understandingdata.com/wp-content/uploads/2019/11/prospecting-seo-clients-with-python-370x370.png 370w, https://understandingdata.com/wp-content/uploads/2019/11/prospecting-seo-clients-with-python-150x150.png 150w, https://understandingdata.com/wp-content/uploads/2019/11/prospecting-seo-clients-with-python-300x300.png 300w, https://understandingdata.com/wp-content/uploads/2019/11/prospecting-seo-clients-with-python-768x768.png 768w, https://understandingdata.com/wp-content/uploads/2019/11/prospecting-seo-clients-with-python-1024x1024.png 1024w, https://understandingdata.com/wp-content/uploads/2019/11/prospecting-seo-clients-with-python-90x90.png 90w, https://understandingdata.com/wp-content/uploads/2019/11/prospecting-seo-clients-with-python-180x180.png 180w, https://understandingdata.com/wp-content/uploads/2019/11/prospecting-seo-clients-with-python-760x760.png 760w, https://understandingdata.com/wp-content/uploads/2019/11/prospecting-seo-clients-with-python-740x740.png 740w, https://understandingdata.com/wp-content/uploads/2019/11/prospecting-seo-clients-with-python.png 1080w\" width=\"370\"/>, <img alt=\"\" class=\"avatar avatar-35 photo\" height=\"35\" src=\"https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&amp;r=g\" srcset=\"https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=70&amp;r=g 2x\" width=\"35\"/>, <img alt=\"Yay! My next two reading books have arrived. I'm super excited to have a lovely cold winter and get deep into some more statistics and machine learning 😍⠀\n",
      "⠀\n",
      "What's your favourite book?⠀\n",
      "⠀\n",
      "#book #books #bookstagram #bookworm #booklover #bookish #booknerd #bookaddict #bookme #bookshelf #bookaholic #booklove #BookNow #bookphotography #booking #bookstagrammer #bookporn #booklovers #bookstore\" src=\"https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\"/>, <img alt=\"Data lakes commonly referred to as data warehouses are a key ingredient into making data easily accessible. Some might call them data puddles if they're making a joke 😉 ⠀\n",
      "⠀\n",
      "#statistics #machinelearning #datascience #statistics #machinelearning #datascienceenthusiasts #datascienceenthusiast  #datasciencemarket #datasciencejokes #datasciencedc #datasciencechain #datascienceinfluencer\" src=\"https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\"/>, <img alt=\"What a hilarious definition for a Data Scientist 😂😂😂⠀\n",
      "⠀\n",
      "#statistics #machinelearning #datascience #statistics #machinelearning #datascienceenthusiasts #datascienceenthusiast  #datasciencemarket #datasciencejokes #datasciencedc #datasciencechain #datascienceinfluencer\" src=\"https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\"/>, <img alt=\"GHOST DISTRIBUTIONS 👻👻👻? Statistics is meant to be fun. By learning statistics we can turn data --&gt; knowledge --&gt; insight --&gt; action. ⠀\n",
      "⠀\n",
      "Any topic can be learned, you just need to put in the correct amount of work and dedication. ⠀\n",
      "⠀\n",
      "#statistics #machinelearning #datascience #statistics #machinelearning #datascienceenthusiasts #datascienceenthusiast  #datasciencemarket #datasciencejokes #datasciencedc #datasciencechain #datascienceinfluencer\" src=\"https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\"/>, <img alt=\"We Love Coffee + Coding. It's the perfect combination for bringing ideas to life! ☕⠀\n",
      "⠀\n",
      "coffee #coffeelikespb #coffeemonkey #coffeetiptop #coffeegiveaway #coffeeartdaily #coffeespirituality #coffeealley #coffeebucks #coffeecakeandconfessions #coffeecraze #coffeeislyf #CoffeeCreamFrappuccino #coffeepause #programming #programmingproblems #programmingjokes #programmingworks #programmingproblem #programmingfacts\" src=\"https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\"/>, <img alt=\"We Love Data.  Using data in the right way can improve the lives. Whether it's in big data, advertising or business optimisation. Data is everywhere and we believe that the ones that leverage it properly will be the heroes of tomorrow.  #datascience #datascienceID #datascienceenthusiasts #datasciencemajor #datasciencejokes #datascienceeducation #datascienceworkshop #datascienceinfluencer #datasciencedc #datasciencemarket #datascienceenthusiast #AIforGood #techforgood #SDGs #coders #womeninai #womenintech #womenempowerment #womeninstem #machinelearning #datascientist #programmers #programming #coding #webdev #javascript #webdeveloper #webdesigners #webdesign #dataengineer\" src=\"https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\"/>, <img alt=\"From performing machine learning on BuzzSumo data. Here are the key insights that I discovered! View the article in my bio for more. information :) #digitalmarketing #digitalmarketingexpert #digitalmarketingbootcamp #digitalmarketingworkshop #digitalmarketinginstitute #digitalmarketingsimplified #DigitalMarketinginDubai #digitalmarketingforfree #digitalmarketingforattorneys #digitalmarketingmoment #DigitalMarketinginPenang #digitalmarketingplan #digitalmarketingstats #digitalmarketingstrategies #digitalmarketing2018 #digitalmarketingquotes #Digitalmarketingcompanyinchennai #digitalmarketingfreak #digitalmarketingspecialist #digitalmarketingarticle #digitalmarketingi #digitalmarketingcertificate #digitalmarketingjobs #digitalmarketingmedia #DigitalMarketingROI #digitalmarketingitalia #Digitalmarketingsingapore #digitalmarketingstrategist #digitalmarketingindustry #digitalmarketingcompany\" src=\"https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\"/>, <img alt=\"Content creation is a time consuming and valuable activity. A marketers’ time for producing content is a precious resource and needs to be effectively optimized.  But is it possible for us to predict which articles will receive more shares than other articles by using machine learning and python? \n",
      "See my latest article on how I start using machine learning techniques to predict article shares with BuzzSumo Data. ❤️ https://understandingdata.com/predicting-article-shares-with-machine-learning/  #datascience #machinelearning #digitalmarketing #ml #data\" src=\"https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\"/>]\n",
      "[<img alt=\"Just Understanding Data\" height=\"136\" src=\"//understandingdata.com/wp-content/uploads/2019/04/cropped-logo_transparent-1.png\" width=\"1200\"/>, <img alt=\"Just Understanding Data\" height=\"136\" src=\"//understandingdata.com/wp-content/uploads/2019/04/cropped-logo_transparent-1.png\" width=\"1200\"/>, <img alt=\"data engineering services\" class=\"wp-image-2396\" sizes=\"(max-width: 640px) 100vw, 640px\" src=\"https://understandingdata.com/wp-content/uploads/2020/05/data-engineering-services.jpg\" srcset=\"https://understandingdata.com/wp-content/uploads/2020/05/data-engineering-services.jpg 640w, https://understandingdata.com/wp-content/uploads/2020/05/data-engineering-services-300x199.jpg 300w, https://understandingdata.com/wp-content/uploads/2020/05/data-engineering-services-370x246.jpg 370w\"/>, <img alt=\"\" src=\"https://understandingdata.com/wp-content/uploads/2020/05/data_pipelines.png\" style=\"object-position:50% 50%\"/>, <img alt=\"data science services\" src=\"https://understandingdata.com/wp-content/uploads/2020/05/041-filter.png\"/>, <img alt=\"engineering with data \" src=\"https://understandingdata.com/wp-content/uploads/2020/05/016-laptop-2.png\"/>, <img alt=\"scalability\" src=\"https://understandingdata.com/wp-content/uploads/2020/05/006-database-1.png\"/>, <img alt=\"\" class=\"wp-image-2516\" sizes=\"(max-width: 900px) 100vw, 900px\" src=\"https://understandingdata.com/wp-content/uploads/2020/05/data_engineering_service.jpg\" srcset=\"https://understandingdata.com/wp-content/uploads/2020/05/data_engineering_service.jpg 900w, https://understandingdata.com/wp-content/uploads/2020/05/data_engineering_service-300x205.jpg 300w, https://understandingdata.com/wp-content/uploads/2020/05/data_engineering_service-768x526.jpg 768w, https://understandingdata.com/wp-content/uploads/2020/05/data_engineering_service-370x253.jpg 370w, https://understandingdata.com/wp-content/uploads/2020/05/data_engineering_service-760x520.jpg 760w, https://understandingdata.com/wp-content/uploads/2020/05/data_engineering_service-740x506.jpg 740w\"/>, <img alt=\"Yay! My next two reading books have arrived. I'm super excited to have a lovely cold winter and get deep into some more statistics and machine learning 😍⠀\n",
      "⠀\n",
      "What's your favourite book?⠀\n",
      "⠀\n",
      "#book #books #bookstagram #bookworm #booklover #bookish #booknerd #bookaddict #bookme #bookshelf #bookaholic #booklove #BookNow #bookphotography #booking #bookstagrammer #bookporn #booklovers #bookstore\" src=\"https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\"/>, <img alt=\"Data lakes commonly referred to as data warehouses are a key ingredient into making data easily accessible. Some might call them data puddles if they're making a joke 😉 ⠀\n",
      "⠀\n",
      "#statistics #machinelearning #datascience #statistics #machinelearning #datascienceenthusiasts #datascienceenthusiast  #datasciencemarket #datasciencejokes #datasciencedc #datasciencechain #datascienceinfluencer\" src=\"https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\"/>, <img alt=\"What a hilarious definition for a Data Scientist 😂😂😂⠀\n",
      "⠀\n",
      "#statistics #machinelearning #datascience #statistics #machinelearning #datascienceenthusiasts #datascienceenthusiast  #datasciencemarket #datasciencejokes #datasciencedc #datasciencechain #datascienceinfluencer\" src=\"https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\"/>, <img alt=\"GHOST DISTRIBUTIONS 👻👻👻? Statistics is meant to be fun. By learning statistics we can turn data --&gt; knowledge --&gt; insight --&gt; action. ⠀\n",
      "⠀\n",
      "Any topic can be learned, you just need to put in the correct amount of work and dedication. ⠀\n",
      "⠀\n",
      "#statistics #machinelearning #datascience #statistics #machinelearning #datascienceenthusiasts #datascienceenthusiast  #datasciencemarket #datasciencejokes #datasciencedc #datasciencechain #datascienceinfluencer\" src=\"https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\"/>, <img alt=\"We Love Coffee + Coding. It's the perfect combination for bringing ideas to life! ☕⠀\n",
      "⠀\n",
      "coffee #coffeelikespb #coffeemonkey #coffeetiptop #coffeegiveaway #coffeeartdaily #coffeespirituality #coffeealley #coffeebucks #coffeecakeandconfessions #coffeecraze #coffeeislyf #CoffeeCreamFrappuccino #coffeepause #programming #programmingproblems #programmingjokes #programmingworks #programmingproblem #programmingfacts\" src=\"https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\"/>, <img alt=\"We Love Data.  Using data in the right way can improve the lives. Whether it's in big data, advertising or business optimisation. Data is everywhere and we believe that the ones that leverage it properly will be the heroes of tomorrow.  #datascience #datascienceID #datascienceenthusiasts #datasciencemajor #datasciencejokes #datascienceeducation #datascienceworkshop #datascienceinfluencer #datasciencedc #datasciencemarket #datascienceenthusiast #AIforGood #techforgood #SDGs #coders #womeninai #womenintech #womenempowerment #womeninstem #machinelearning #datascientist #programmers #programming #coding #webdev #javascript #webdeveloper #webdesigners #webdesign #dataengineer\" src=\"https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\"/>, <img alt=\"From performing machine learning on BuzzSumo data. Here are the key insights that I discovered! View the article in my bio for more. information :) #digitalmarketing #digitalmarketingexpert #digitalmarketingbootcamp #digitalmarketingworkshop #digitalmarketinginstitute #digitalmarketingsimplified #DigitalMarketinginDubai #digitalmarketingforfree #digitalmarketingforattorneys #digitalmarketingmoment #DigitalMarketinginPenang #digitalmarketingplan #digitalmarketingstats #digitalmarketingstrategies #digitalmarketing2018 #digitalmarketingquotes #Digitalmarketingcompanyinchennai #digitalmarketingfreak #digitalmarketingspecialist #digitalmarketingarticle #digitalmarketingi #digitalmarketingcertificate #digitalmarketingjobs #digitalmarketingmedia #DigitalMarketingROI #digitalmarketingitalia #Digitalmarketingsingapore #digitalmarketingstrategist #digitalmarketingindustry #digitalmarketingcompany\" src=\"https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\"/>, <img alt=\"Content creation is a time consuming and valuable activity. A marketers’ time for producing content is a precious resource and needs to be effectively optimized.  But is it possible for us to predict which articles will receive more shares than other articles by using machine learning and python? \n",
      "See my latest article on how I start using machine learning techniques to predict article shares with BuzzSumo Data. ❤️ https://understandingdata.com/predicting-article-shares-with-machine-learning/  #datascience #machinelearning #digitalmarketing #ml #data\" src=\"https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\"/>]\n",
      "[<img alt=\"Wifi signal with exclamation mark through it\" class=\"main-nav__logo\" src=\"/img/interneting-is-hard-logo-97b225.svg\"/>, <img alt=\"Diagram: image pointing to &lt;img&gt; element in a web page and HTML link pointing from web page to another web page\" class=\"diagram__image\" height=\"230\" src=\"/html-and-css/links-and-images/links-and-images-6820c7.png\" width=\"511\"/>, <img alt=\"Diagram: the 7 project files for this project (3 HTML files, 4 image files)\" class=\"diagram__image\" height=\"297\" src=\"/html-and-css/links-and-images/links-and-images-example-7ab70f.png\" width=\"480\"/>, <img alt=\"Screenshot: Atom’s file browser after creating the example files\" class=\"diagram__image\" height=\"316\" src=\"/html-and-css/links-and-images/example-files-b21613.png\" width=\"241\"/>, <img alt=\"Diagram: HTML attribute attached to an HTML element, which is attached to raw content\" class=\"diagram__image\" height=\"293\" src=\"/html-and-css/links-and-images/html-attributes-6f5690.png\" width=\"279\"/>, <img alt=\"Web page with a blue &lt;a href&gt; link in it\" class=\"diagram__image\" height=\"187\" src=\"/html-and-css/links-and-images/html-link-href-element-61348e.png\" width=\"404\"/>, <img alt=\"Diagram: absolute links (pointing to other website), relative links (pointing to other page in same website), and root-relative links (pointing to other page in same site via its domain)\" class=\"diagram__image\" height=\"226\" src=\"/html-and-css/links-and-images/absolute-relative-root-relative-links-104560.png\" width=\"580\"/>, <img alt=\"Diagram: scheme (https://), domain (developer.mozilla.org), path (/en-us/docs/web/html)\" class=\"diagram__image\" height=\"97\" src=\"/html-and-css/links-and-images/absolute-link-syntax-64d730.png\" width=\"580\"/>, <img alt=\"Diagram: absolute link pointing from our web page to another website\" class=\"diagram__image\" height=\"226\" src=\"/html-and-css/links-and-images/absolute-links-32f469.png\" width=\"580\"/>, <img alt=\"Diagram: relative link pointing from one page in our website to another one\" class=\"diagram__image\" height=\"318\" src=\"/html-and-css/links-and-images/relative-links-e178d0.png\" width=\"580\"/>, <img alt=\"Diagram: attempting to link to the wrong HTML page due to a missing parent reference\" class=\"diagram__image\" height=\"444\" src=\"/html-and-css/links-and-images/relative-link-no-parent-4629d0.png\" width=\"420\"/>, <img alt=\"Diagram: linking to the correct HTML page after adding parent reference\" class=\"diagram__image\" height=\"318\" src=\"/html-and-css/links-and-images/relative-link-with-parent-666b79.png\" width=\"420\"/>, <img alt=\"Diagram: a link from one web page in our site traveling through our domain, then to another page in our site\" class=\"diagram__image\" height=\"226\" src=\"/html-and-css/links-and-images/root-relative-links-368060.png\" width=\"580\"/>, <img alt=\"Diagram: JPG, GIF, PNG, and SVG example images\" class=\"diagram__image\" height=\"155\" src=\"/html-and-css/links-and-images/image-formats-62b23d.png\" width=\"500\"/>, <img alt=\"a mochi in a transparent bubble that has a gradient on it\" class=\"diagram__image\" height=\"100\" src=\"/html-and-css/links-and-images/mochi-77c69d.jpg\" width=\"100\"/>, <img alt=\"an animated mochi bouncing up and down\" class=\"diagram__image\" height=\"75\" src=\"/html-and-css/links-and-images/mochi-961ee5.gif\" width=\"75\"/>, <img alt=\"a plain old mochi\" class=\"diagram__image\" height=\"100\" src=\"/html-and-css/links-and-images/mochi-f95fdf.png\" width=\"100\"/>, <img alt=\"SVG image with crisp edges compared to a pixelated PNG image\" class=\"diagram__image\" height=\"272\" src=\"/html-and-css/links-and-images/svg-vs-png-image-3df7fd.png\" width=\"600\"/>, <img alt=\"Web page after embedding JPG, GIF, PNG, and SVG &lt;img&gt; elements\" class=\"diagram__image\" height=\"852\" src=\"/html-and-css/links-and-images/html-img-formats-3b43c6.png\" width=\"404\"/>, <img alt=\"Web page rendering gibberish for international characters\" class=\"diagram__image\" height=\"306\" src=\"/html-and-css/links-and-images/html-character-sets-no-utf-8-304820.png\" width=\"404\"/>, <img alt=\"Wi-fi signal with exclamation mark through it\" class=\"about__logo\" src=\"/img/interneting-is-hard-logo-97b225.svg\"/>, <img alt=\"Twitter logo\" class=\"about__twitter\" src=\"/img/twitter-icon-29cd78.png\"/>, <img alt=\"Facebook logo\" class=\"about__facebook\" src=\"/img/facebook-icon-e6def5.png\"/>, <img alt=\"Envelope icon\" class=\"about__email\" src=\"/img/email-icon-a4c505.png\"/>]\n"
     ]
    }
   ],
   "source": [
    "for key, value in url_dictionary.items():\n",
    "    if len(value) > 0:\n",
    "        print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An easier way to write the above code would be via a dictionary comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dictionary = {key: value for key, value in url_dictionary.items() if len(value) > 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now clean all of the image URLs inside of every dictionary key and change all of the relative URL paths to exact URL paths.\n",
    "\n",
    "Let's start by printing out all of the different image sources to see how we might need to clean up the data below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "//understandingdata.com/wp-content/uploads/2019/04/cropped-logo_transparent-1.png\n",
      "//understandingdata.com/wp-content/uploads/2019/04/cropped-logo_transparent-1.png\n",
      "https://understandingdata.com/wp-content/uploads/2019/09/james-anthony-phoenix.jpg\n",
      "https://understandingdata.com/wp-content/uploads/2019/09/james-anthony-phoenix.jpg\n",
      "https://understandingdata.com/wp-content/uploads/2020/05/Is-Web-Scraping-Illegal-370x370.png\n",
      "https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://understandingdata.com/wp-content/uploads/2020/03/web-scraping-tools-370x192.jpg\n",
      "https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://understandingdata.com/wp-content/uploads/2020/03/community-detection-370x238.png\n",
      "https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://understandingdata.com/wp-content/uploads/2020/03/what-is-web-scraping-370x370.png\n",
      "https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://understandingdata.com/wp-content/uploads/2020/03/installing-chromedriver-headless-browser.png\n",
      "https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://understandingdata.com/wp-content/uploads/2020/02/how-to-install-python-anaconda-mac-370x208.png\n",
      "https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://understandingdata.com/wp-content/uploads/2019/12/ninjaoutreach-black-friday-deal-370x194.png\n",
      "https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://understandingdata.com/wp-content/uploads/2020/02/webscraping_advantages_disadvantages-370x370.png\n",
      "https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://understandingdata.com/wp-content/uploads/2019/11/prospecting-seo-clients-with-python-370x370.png\n",
      "https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "//understandingdata.com/wp-content/uploads/2019/04/cropped-logo_transparent-1.png\n",
      "//understandingdata.com/wp-content/uploads/2019/04/cropped-logo_transparent-1.png\n",
      "https://understandingdata.com/wp-content/uploads/2020/05/data-engineering-services.jpg\n",
      "https://understandingdata.com/wp-content/uploads/2020/05/data_pipelines.png\n",
      "https://understandingdata.com/wp-content/uploads/2020/05/041-filter.png\n",
      "https://understandingdata.com/wp-content/uploads/2020/05/016-laptop-2.png\n",
      "https://understandingdata.com/wp-content/uploads/2020/05/006-database-1.png\n",
      "https://understandingdata.com/wp-content/uploads/2020/05/data_engineering_service.jpg\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "/img/interneting-is-hard-logo-97b225.svg\n",
      "/html-and-css/links-and-images/links-and-images-6820c7.png\n",
      "/html-and-css/links-and-images/links-and-images-example-7ab70f.png\n",
      "/html-and-css/links-and-images/example-files-b21613.png\n",
      "/html-and-css/links-and-images/html-attributes-6f5690.png\n",
      "/html-and-css/links-and-images/html-link-href-element-61348e.png\n",
      "/html-and-css/links-and-images/absolute-relative-root-relative-links-104560.png\n",
      "/html-and-css/links-and-images/absolute-link-syntax-64d730.png\n",
      "/html-and-css/links-and-images/absolute-links-32f469.png\n",
      "/html-and-css/links-and-images/relative-links-e178d0.png\n",
      "/html-and-css/links-and-images/relative-link-no-parent-4629d0.png\n",
      "/html-and-css/links-and-images/relative-link-with-parent-666b79.png\n",
      "/html-and-css/links-and-images/root-relative-links-368060.png\n",
      "/html-and-css/links-and-images/image-formats-62b23d.png\n",
      "/html-and-css/links-and-images/mochi-77c69d.jpg\n",
      "/html-and-css/links-and-images/mochi-961ee5.gif\n",
      "/html-and-css/links-and-images/mochi-f95fdf.png\n",
      "/html-and-css/links-and-images/svg-vs-png-image-3df7fd.png\n",
      "/html-and-css/links-and-images/html-img-formats-3b43c6.png\n",
      "/html-and-css/links-and-images/html-character-sets-no-utf-8-304820.png\n",
      "/img/interneting-is-hard-logo-97b225.svg\n",
      "/img/twitter-icon-29cd78.png\n",
      "/img/facebook-icon-e6def5.png\n",
      "/img/email-icon-a4c505.png\n"
     ]
    }
   ],
   "source": [
    "for key, value in cleaned_dictionary.items():\n",
    "    for item in value:\n",
    "        print(item.attrs['src'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the scope of this tutorial, I have decided to:\n",
    "    \n",
    "- Remove the logo links with the //\n",
    "- Add on the domain to the relative URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = []\n",
    "\n",
    "for key, images in cleaned_dictionary.items():\n",
    "    \n",
    "    clean_urls = []\n",
    "    domain_name = tldextract.extract(key).registered_domain\n",
    "    \n",
    "    for image in images:\n",
    "        \n",
    "        source_image_url = image.attrs['src']\n",
    "        \n",
    "        if source_image_url.startswith(\"//\"):\n",
    "            pass\n",
    "        elif domain_name not in source_image_url and 'http' not in source_image_url: \n",
    "            url = \"https://\" + domain_name + source_image_url\n",
    "            all_images.append(url)\n",
    "        else:\n",
    "            all_images.append(source_image_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://understandingdata.com/wp-content/uploads/2019/09/james-anthony-phoenix.jpg',\n",
       " 'https://understandingdata.com/wp-content/uploads/2019/09/james-anthony-phoenix.jpg',\n",
       " 'https://understandingdata.com/wp-content/uploads/2020/05/Is-Web-Scraping-Illegal-370x370.png',\n",
       " 'https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g',\n",
       " 'https://understandingdata.com/wp-content/uploads/2020/03/web-scraping-tools-370x192.jpg',\n",
       " 'https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g',\n",
       " 'https://understandingdata.com/wp-content/uploads/2020/03/community-detection-370x238.png',\n",
       " 'https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g',\n",
       " 'https://understandingdata.com/wp-content/uploads/2020/03/what-is-web-scraping-370x370.png',\n",
       " 'https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g',\n",
       " 'https://understandingdata.com/wp-content/uploads/2020/03/installing-chromedriver-headless-browser.png',\n",
       " 'https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g',\n",
       " 'https://understandingdata.com/wp-content/uploads/2020/02/how-to-install-python-anaconda-mac-370x208.png',\n",
       " 'https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g',\n",
       " 'https://understandingdata.com/wp-content/uploads/2019/12/ninjaoutreach-black-friday-deal-370x194.png',\n",
       " 'https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g',\n",
       " 'https://understandingdata.com/wp-content/uploads/2020/02/webscraping_advantages_disadvantages-370x370.png',\n",
       " 'https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g',\n",
       " 'https://understandingdata.com/wp-content/uploads/2019/11/prospecting-seo-clients-with-python-370x370.png',\n",
       " 'https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g',\n",
       " 'https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png',\n",
       " 'https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png',\n",
       " 'https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png',\n",
       " 'https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png',\n",
       " 'https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png',\n",
       " 'https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png',\n",
       " 'https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png',\n",
       " 'https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png',\n",
       " 'https://understandingdata.com/wp-content/uploads/2020/05/data-engineering-services.jpg',\n",
       " 'https://understandingdata.com/wp-content/uploads/2020/05/data_pipelines.png',\n",
       " 'https://understandingdata.com/wp-content/uploads/2020/05/041-filter.png',\n",
       " 'https://understandingdata.com/wp-content/uploads/2020/05/016-laptop-2.png',\n",
       " 'https://understandingdata.com/wp-content/uploads/2020/05/006-database-1.png',\n",
       " 'https://understandingdata.com/wp-content/uploads/2020/05/data_engineering_service.jpg',\n",
       " 'https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png',\n",
       " 'https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png',\n",
       " 'https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png',\n",
       " 'https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png',\n",
       " 'https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png',\n",
       " 'https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png',\n",
       " 'https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png',\n",
       " 'https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png',\n",
       " 'https://internetingishard.com/img/interneting-is-hard-logo-97b225.svg',\n",
       " 'https://internetingishard.com/html-and-css/links-and-images/links-and-images-6820c7.png',\n",
       " 'https://internetingishard.com/html-and-css/links-and-images/links-and-images-example-7ab70f.png',\n",
       " 'https://internetingishard.com/html-and-css/links-and-images/example-files-b21613.png',\n",
       " 'https://internetingishard.com/html-and-css/links-and-images/html-attributes-6f5690.png',\n",
       " 'https://internetingishard.com/html-and-css/links-and-images/html-link-href-element-61348e.png',\n",
       " 'https://internetingishard.com/html-and-css/links-and-images/absolute-relative-root-relative-links-104560.png',\n",
       " 'https://internetingishard.com/html-and-css/links-and-images/absolute-link-syntax-64d730.png',\n",
       " 'https://internetingishard.com/html-and-css/links-and-images/absolute-links-32f469.png',\n",
       " 'https://internetingishard.com/html-and-css/links-and-images/relative-links-e178d0.png',\n",
       " 'https://internetingishard.com/html-and-css/links-and-images/relative-link-no-parent-4629d0.png',\n",
       " 'https://internetingishard.com/html-and-css/links-and-images/relative-link-with-parent-666b79.png',\n",
       " 'https://internetingishard.com/html-and-css/links-and-images/root-relative-links-368060.png',\n",
       " 'https://internetingishard.com/html-and-css/links-and-images/image-formats-62b23d.png',\n",
       " 'https://internetingishard.com/html-and-css/links-and-images/mochi-77c69d.jpg',\n",
       " 'https://internetingishard.com/html-and-css/links-and-images/mochi-961ee5.gif',\n",
       " 'https://internetingishard.com/html-and-css/links-and-images/mochi-f95fdf.png',\n",
       " 'https://internetingishard.com/html-and-css/links-and-images/svg-vs-png-image-3df7fd.png',\n",
       " 'https://internetingishard.com/html-and-css/links-and-images/html-img-formats-3b43c6.png',\n",
       " 'https://internetingishard.com/html-and-css/links-and-images/html-character-sets-no-utf-8-304820.png',\n",
       " 'https://internetingishard.com/img/interneting-is-hard-logo-97b225.svg',\n",
       " 'https://internetingishard.com/img/twitter-icon-29cd78.png',\n",
       " 'https://internetingishard.com/img/facebook-icon-e6def5.png',\n",
       " 'https://internetingishard.com/img/email-icon-a4c505.png']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cleaning the image URLs, we can now refer to method one for downloading the images to our computer! \n",
    "\n",
    "This time let's convert it into a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_images(image_urls_list:list, directory_path):\n",
    "    \n",
    "    os.chdir(directory_path)\n",
    "    \n",
    "    for img in image_urls_list:\n",
    "        file_name = img.split('/')[-1]\n",
    "        \n",
    "        url_paths_to_try = [img, img.replace('https://', 'https://www.')]\n",
    "        \n",
    "        for url_image_path in url_paths_to_try:\n",
    "            try:\n",
    "                r = requests.get(img, stream=True)\n",
    "                if r.status_code == 200:\n",
    "                    with open(file_name, 'wb') as f:\n",
    "                        for chunk in r:\n",
    "                            f.write(chunk)\n",
    "            except Exception as e:\n",
    "                pass   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/jamesaphoenix/Desktop/Imran_And_James/Python_For_SEO/6_downloading_multiple_images/all_images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_images(image_urls_list=all_images, directory_path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantastic! \n",
    "\n",
    "Now there are some things that we didn't necessarily cover for which include:\n",
    "\n",
    "- http:// only image urls.\n",
    "- http://www. only image urls.\n",
    "\n",
    "But for the most part, you'll be able to download images in bulk!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![how to download multiple images within python](https://sempioneer.com/wp-content/uploads/2020/06/all_images.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How To Speed Up Your Image Downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its important when working with 100's or 1000's of URLs to avoid using as synchronous approach to downloading images. An asynchronous approach means that we can download multiple web pages or multiple images in parallel.\n",
    "\n",
    "<strong> This means that the overall execution time will be much quicker! </strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ThreadPoolExecutor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ThreadPoolExecutor is one of python's built in I/O packages for creating an asynchronous behaviour via multiple threads. In order to utilise it, we will make sure that the function will only work on a single URL.\n",
    "\n",
    "Then we will pass the image URL list into multiple workers ;) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_single_image(img):\n",
    "    file_name = img.split('/')[-1]\n",
    "    \n",
    "    # Let's try both of these versions in a loop [https:// and https://www.]\n",
    "    url_paths_to_try = [img, img.replace('https://', 'https://www.')]\n",
    "    for url_image_path in url_paths_to_try:\n",
    "        try:\n",
    "            r = requests.get(img, stream=True)\n",
    "            if r.status_code == 200:\n",
    "                with open(file_name, 'wb') as f:\n",
    "                    for chunk in r:\n",
    "                        f.write(chunk)\n",
    "            return \"Completed\"\n",
    "        except Exception as e:\n",
    "            return \"Failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://understandingdata.com/wp-content/uploads/2019/09/james-anthony-phoenix.jpg',\n",
       " 'https://understandingdata.com/wp-content/uploads/2019/09/james-anthony-phoenix.jpg',\n",
       " 'https://understandingdata.com/wp-content/uploads/2020/05/Is-Web-Scraping-Illegal-370x370.png',\n",
       " 'https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g',\n",
       " 'https://understandingdata.com/wp-content/uploads/2020/03/web-scraping-tools-370x192.jpg']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_images[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code will create a new directory and then make it the current active working directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('/Users/jamesaphoenix/Desktop/Imran_And_James/Python_For_SEO/6_downloading_multiple_images/all_images_asnyc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/jamesaphoenix/Desktop/Imran_And_James/Python_For_SEO/6_downloading_multiple_images/all_images_asnyc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    future_to_url = {executor.submit(extract_single_image, image_url) for image_url in all_images}\n",
    "    \n",
    "    for future in concurrent.futures.as_completed(future_to_url):\n",
    "        try:\n",
    "            url = future_to_url[future]\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        try:\n",
    "            data = future.result()\n",
    "        except Exception as exc:\n",
    "            print('%r generated an exception: %s' % (url, exc))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should've downloaded the images but at a much faster rate! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Async Programming! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like JavaScript, Python 3.6+ comes bundled with native support for co-routines called [asyncio](https://docs.python.org/3/library/asyncio.html). Similar to NodeJS, there is a method available to you for creating custom event loops for async code. \n",
    "\n",
    "We will also need to download an async code HTTP requests library called [aiohttp](https://docs.aiohttp.org/en/stable/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.7/site-packages (3.6.2)\n",
      "Requirement already satisfied: chardet<4.0,>=2.0 in /opt/anaconda3/lib/python3.7/site-packages (from aiohttp) (3.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.7/site-packages (from aiohttp) (1.4.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.7/site-packages (from aiohttp) (19.3.0)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /opt/anaconda3/lib/python3.7/site-packages (from aiohttp) (3.0.1)\n",
      "Requirement already satisfied: multidict<5.0,>=4.5 in /opt/anaconda3/lib/python3.7/site-packages (from aiohttp) (4.7.6)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/anaconda3/lib/python3.7/site-packages (from yarl<2.0,>=1.0->aiohttp) (2.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install aiohttp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also download aiofiles that allows us to write multiple image files asynchronously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aiofiles in /opt/anaconda3/lib/python3.7/site-packages (0.5.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install aiofiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import aiofiles\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    " os.mkdir('/Users/jamesaphoenix/Desktop/Imran_And_James/Python_For_SEO/6_downloading_multiple_images/all_images_async_event_loop')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/jamesaphoenix/Desktop/Imran_And_James/Python_For_SEO/6_downloading_multiple_images/all_images_async_event_loop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How To Download 1 File Asychronously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://understandingdata.com/wp-content/uploads/2019/09/james-anthony-phoenix.jpg']\n"
     ]
    }
   ],
   "source": [
    "print(all_images[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_image = all_images[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://understandingdata.com/wp-content/uploads/2019/09/james-anthony-phoenix.jpg'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "async with aiohttp.ClientSession() as session:\n",
    "    async with session.get(single_image[0]) as resp:\n",
    "        # 1. Capturing the image file name like we did before:\n",
    "        single_image_name = single_image[0].split('/')[-1]\n",
    "        # 2. Only proceed further if the HTTP response is 200 (Ok)\n",
    "        if resp.status == 200:\n",
    "            async with aiofiles.open(single_image_name, mode='wb') as f:\n",
    "                await f.write(await resp.read())\n",
    "                await f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Downloading one image with aiofiles](https://sempioneer.com/wp-content/uploads/2020/06/image_files.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We will need to structure our code slightly different for the async version to work across multiple files:\n",
    "\n",
    "1. We will have a fetch function to query every image URL.\n",
    "2. We will have a main function that creates, then executes a series of co-routines.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch(session, url):\n",
    "    async with session.get(url) as resp:\n",
    "        # 1. Capturing the image file name like we did before:\n",
    "        url_name = url.split('/')[-1]\n",
    "        # 2. Only proceed further if the HTTP response is 200 (Ok)\n",
    "        if resp.status == 200:\n",
    "            async with aiofiles.open(url_name, mode='wb') as f:\n",
    "                await f.write(await resp.read())\n",
    "                await f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main(image_urls:list):\n",
    "    tasks = []\n",
    "    headers = {\n",
    "        \"user-agent\": \"Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)\"}\n",
    "    async with aiohttp.ClientSession(headers=headers) as session:\n",
    "        for image in image_urls:\n",
    "            tasks.append(await fetch(session, url))\n",
    "    data = await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object main at 0x10bed09e0>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main(all_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-1d6e7fd07cf2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/asyncio/runners.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_running_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         raise RuntimeError(\n\u001b[0;32m---> 34\u001b[0;31m             \"asyncio.run() cannot be called from a running event loop\")\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcoroutines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miscoroutine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "asyncio.run(main(all_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "☝️☝️☝️ Notice how when we call this function, it doesn't actually run and produces a [co-routine!](https://docs.python.org/3/library/asyncio-task.html) ☝️☝️☝️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use asyncio as method for executing all of the fetch callables that need to be completed:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Error with asyncio.run](https://sempioneer.com/wp-content/uploads/2020/06/error-downloading-python-files.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you receive this type of error when running the following command:\n",
    "\n",
    "~~~\n",
    "\n",
    "asyncio.run(main(all_images))\n",
    "\n",
    "~~~\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<strong> It is likely because you're trying to run asyncio within an event loop which is not natively possible. (Jupyter notebook runs in an event loop!). </strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How To Download Multiple Python Files Inside Of A Python File (.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the variable containing our URLs to a .txt file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('images.txt', 'w') as f:\n",
    "    for item in all_images:\n",
    "        f.write(f\"{item}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('images.txt', 'r') as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create A Python File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you will need to create a python file and add the following code to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then run the python script in <strong> either your terminal / command line with: </strong>\n",
    "    \n",
    "    \n",
    "~~~\n",
    "\n",
    "python3 python_file_name.py\n",
    "\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down what's happening in the above code snippet:\n",
    "    \n",
    "1. We are importing all of the relevant packages for async programming with files.\n",
    "2. Then we create a new directory.\n",
    "3. After creating the new folder we change that folder to be the active working directory.\n",
    "4. We then read the variable data which was previously saved from the file called images.txt\n",
    "5. Then we create a series of co-routines and execute them within a main() function with asyncio.\n",
    "6. As these co-routines are executed every file is asynchronously saved to your computer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![downloading multiple files with asyncio-aiohttp](https://sempioneer.com/wp-content/uploads/2020/06/asyncio-with-aiofiles.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's clear up and delete all of the folders to clean up our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_paths = ['/Users/jamesaphoenix/Desktop/Imran_And_James/Python_For_SEO/6_downloading_multiple_images/all_images',\n",
    "            '/Users/jamesaphoenix/Desktop/Imran_And_James/Python_For_SEO/6_downloading_multiple_images/all_images_async_event_loop',\n",
    "            '/Users/jamesaphoenix/Desktop/Imran_And_James/Python_For_SEO/6_downloading_multiple_images/all_images_async_event_loop'\n",
    "            \n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jamesaphoenix/Desktop/Imran_And_James/Python_For_SEO/6_downloading_multiple_images/all_images_async_event_loop\r\n"
     ]
    }
   ],
   "source": [
    "!pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether you decide to download images synchronously or asynchronously, its important to realise that although you can do this in tools such as ScreamingFrog or with Google Chrome Extensions. Being able to download images with python allows you to extend your automation capabilities and what other programs, APIs etc you might use that image data with! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
