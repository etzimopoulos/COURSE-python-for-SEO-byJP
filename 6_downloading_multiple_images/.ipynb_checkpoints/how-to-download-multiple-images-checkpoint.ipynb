{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. How To Download Multiple Images In Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Outcomes\n",
    "\n",
    "- To learn how to download multiple images in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its a great skill to be able to automatically download all of the images across Xn HTML pages. So in this guide you'll learn two methods for extracting all of the images that a website has on its pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin with the easiest of the two methods, if you already have a list of image URLs then we can follow this process:\n",
    "\n",
    "1. Change into a directory where we would like to store all of the images.\n",
    "2. Make a request to download all of the images, one by one.\n",
    "3. We will also add in error handling so that if a URL no longer exists the code will still work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tldextract in /opt/anaconda3/lib/python3.7/site-packages (2.2.2)\n",
      "Requirement already satisfied: requests>=2.1.0 in /opt/anaconda3/lib/python3.7/site-packages (from tldextract) (2.22.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.7/site-packages (from tldextract) (46.0.0.post20200309)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.7/site-packages (from tldextract) (2.8)\n",
      "Requirement already satisfied: requests-file>=1.4 in /opt/anaconda3/lib/python3.7/site-packages (from tldextract) (1.5.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda3/lib/python3.7/site-packages (from requests>=2.1.0->tldextract) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.7/site-packages (from requests>=2.1.0->tldextract) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.7/site-packages (from requests>=2.1.0->tldextract) (3.0.4)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.7/site-packages (from requests-file>=1.4->tldextract) (1.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tldextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import subprocess\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import tldextract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir all_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mall_images\u001b[m\u001b[m                            how-to-download-multiple-images.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing into the directory of the folder called all_images, this can be done by either:\n",
    "\n",
    "~~~\n",
    "\n",
    "cd all_images\n",
    "os.chdir('path')\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('all_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jamesaphoenix/Desktop/Imran_And_James/Python_For_SEO/6_downloading_multiple_images/all_images\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method One: How To Download Multiple Images From A Python List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create a python list to store any images that didn't have a 200 status code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "broken_images = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_urls = ['https://sempioneer.com/wp-content/uploads/2020/05/dataframe-300x84.png',\n",
    "             'https://sempioneer.com/wp-content/uploads/2020/05/json_format_data-300x72.png']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the file name: dataframe-300x84.png\n",
      "This is the file name: json_format_data-300x72.png\n"
     ]
    }
   ],
   "source": [
    "for img in image_urls:\n",
    "    # We can split the file based upon / and extract the last split within the python list below:\n",
    "    file_name = img.split('/')[-1]\n",
    "    print(f\"This is the file name: {file_name}\")\n",
    "    # Now let's send a request to the image URL:\n",
    "    r = requests.get(img, stream=True)\n",
    "    # We can check that the status code is 200 before doing anything else:\n",
    "    if r.status_code == 200:\n",
    "        # This command below will allow us to write the data to a file as binary:\n",
    "        with open(file_name, 'wb') as f:\n",
    "            for chunk in r:\n",
    "                f.write(chunk)\n",
    "                \n",
    "    else:\n",
    "        # We will write all of the images back to the broken_images list:\n",
    "        broken_images.append(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "☝️ See how simple that is! ☝️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you check your folder, you will have now downloaded all of the images that contained a status code of 200! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![downloading images correctly with python](https://sempioneer.com/wp-content/uploads/2020/06/how-to-download-images-with-python.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method Two: How To Download Multiple Images From Many HTML Web Pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we don't yet have the exact image URLs, we will need to do the following:\n",
    "\n",
    "1. Download the HTML content of every web page.\n",
    "2. Extract all of the image URLs for every page.\n",
    "3. Create the file names.\n",
    "4. Check to see if the image status code is 200.\n",
    "5. Write all of images to your local computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This website [internetingishard.com](https://www.internetingishard.com/html-and-css/links-and-images/) has some relative image URLs. Therefore we will need to ensure that our code can handle for the following two types of image source URLs:\n",
    "\n",
    "---\n",
    "\n",
    "- Exact Filepath: https://www.internetingishard.com/html-and-css/links-and-images/html-attributes-6f5690.png\n",
    "- Relative Filepath: /html-and-css/links-and-images/html-attributes-6f5690.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_pages = ['https://understandingdata.com/', \n",
    "             'https://understandingdata.com/data-engineering-services/',\n",
    "             'https://www.internetingishard.com/html-and-css/links-and-images/']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also extract the domain of every URL whilst we loop over the webpages like so:\n",
    "    \n",
    "~~~\n",
    "\n",
    "for page in webpages:\n",
    "    domain_name = tldextract.extract(page).registered_domain\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_dictionary = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The domain name: understandingdata.com\n",
      "The domain name: understandingdata.com\n",
      "The domain name: internetingishard.com\n"
     ]
    }
   ],
   "source": [
    "for page in web_pages:\n",
    "    # 1. Extracting the domain name of the web page:\n",
    "    domain_name = tldextract.extract(page).registered_domain\n",
    "    print(f\"The domain name: {domain_name}\")    \n",
    "    # 2. Request the web page:\n",
    "    r = requests.get(page)\n",
    "    # 3. Check to see if the web page returned a status_200:\n",
    "    if r.status_code == 200:\n",
    "        \n",
    "        # 4. Create a URL dictionary entry for future use:\n",
    "        url_dictionary[page] = []\n",
    "        \n",
    "        # 5. Parse the HTML content with BeautifulSoup and look for image tags:\n",
    "        soup = BeautifulSoup(r.content, 'html.parser')\n",
    "        \n",
    "        # 6. Find all of the images per web page:\n",
    "        images = soup.findAll('img')\n",
    "        \n",
    "        # 7. Store all of the images \n",
    "        url_dictionary[page].extend(images)\n",
    "        \n",
    "    else:\n",
    "        print('failed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's double check and filter our dictionary so that we only look at web pages where there was at least 1 image tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This domain: https://understandingdata.com/ has more than 1 image on the web page.\n",
      "This domain: https://understandingdata.com/data-engineering-services/ has more than 1 image on the web page.\n",
      "This domain: https://www.internetingishard.com/html-and-css/links-and-images/ has more than 1 image on the web page.\n"
     ]
    }
   ],
   "source": [
    "for key, value in url_dictionary.items():\n",
    "    if len(value) > 0:\n",
    "        print(f\"This domain: {key} has more than 1 image on the web page.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An easier way to write the above code would be via a dictionary comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dictionary = {key: value for key, value in url_dictionary.items() if len(value) > 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now clean all of the image URLs inside of every dictionary key and change all of the relative URL paths to exact URL paths.\n",
    "\n",
    "Let's start by printing out all of the different image sources to see how we might need to clean up the data below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "//understandingdata.com/wp-content/uploads/2019/04/cropped-logo_transparent-1.png\n",
      "//understandingdata.com/wp-content/uploads/2019/04/cropped-logo_transparent-1.png\n",
      "https://understandingdata.com/wp-content/uploads/2019/09/james-anthony-phoenix.jpg\n",
      "https://understandingdata.com/wp-content/uploads/2019/09/james-anthony-phoenix.jpg\n",
      "https://understandingdata.com/wp-content/uploads/2020/05/Is-Web-Scraping-Illegal-370x370.png\n",
      "https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://understandingdata.com/wp-content/uploads/2020/03/web-scraping-tools-370x192.jpg\n",
      "https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://understandingdata.com/wp-content/uploads/2020/03/community-detection-370x238.png\n",
      "https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://understandingdata.com/wp-content/uploads/2020/03/what-is-web-scraping-370x370.png\n",
      "https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://understandingdata.com/wp-content/uploads/2020/03/installing-chromedriver-headless-browser.png\n",
      "https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://understandingdata.com/wp-content/uploads/2020/02/how-to-install-python-anaconda-mac-370x208.png\n",
      "https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://understandingdata.com/wp-content/uploads/2019/12/ninjaoutreach-black-friday-deal-370x194.png\n",
      "https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://understandingdata.com/wp-content/uploads/2020/02/webscraping_advantages_disadvantages-370x370.png\n",
      "https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://understandingdata.com/wp-content/uploads/2019/11/prospecting-seo-clients-with-python-370x370.png\n",
      "https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "//understandingdata.com/wp-content/uploads/2019/04/cropped-logo_transparent-1.png\n",
      "//understandingdata.com/wp-content/uploads/2019/04/cropped-logo_transparent-1.png\n",
      "https://understandingdata.com/wp-content/uploads/2020/05/data-engineering-services.jpg\n",
      "https://understandingdata.com/wp-content/uploads/2020/05/data_pipelines.png\n",
      "https://understandingdata.com/wp-content/uploads/2020/05/041-filter.png\n",
      "https://understandingdata.com/wp-content/uploads/2020/05/016-laptop-2.png\n",
      "https://understandingdata.com/wp-content/uploads/2020/05/006-database-1.png\n",
      "https://understandingdata.com/wp-content/uploads/2020/05/data_engineering_service.jpg\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "/img/interneting-is-hard-logo-97b225.svg\n",
      "/html-and-css/links-and-images/links-and-images-6820c7.png\n",
      "/html-and-css/links-and-images/links-and-images-example-7ab70f.png\n",
      "/html-and-css/links-and-images/example-files-b21613.png\n",
      "/html-and-css/links-and-images/html-attributes-6f5690.png\n",
      "/html-and-css/links-and-images/html-link-href-element-61348e.png\n",
      "/html-and-css/links-and-images/absolute-relative-root-relative-links-104560.png\n",
      "/html-and-css/links-and-images/absolute-link-syntax-64d730.png\n",
      "/html-and-css/links-and-images/absolute-links-32f469.png\n",
      "/html-and-css/links-and-images/relative-links-e178d0.png\n",
      "/html-and-css/links-and-images/relative-link-no-parent-4629d0.png\n",
      "/html-and-css/links-and-images/relative-link-with-parent-666b79.png\n",
      "/html-and-css/links-and-images/root-relative-links-368060.png\n",
      "/html-and-css/links-and-images/image-formats-62b23d.png\n",
      "/html-and-css/links-and-images/mochi-77c69d.jpg\n",
      "/html-and-css/links-and-images/mochi-961ee5.gif\n",
      "/html-and-css/links-and-images/mochi-f95fdf.png\n",
      "/html-and-css/links-and-images/svg-vs-png-image-3df7fd.png\n",
      "/html-and-css/links-and-images/html-img-formats-3b43c6.png\n",
      "/html-and-css/links-and-images/html-character-sets-no-utf-8-304820.png\n",
      "/img/interneting-is-hard-logo-97b225.svg\n",
      "/img/twitter-icon-29cd78.png\n",
      "/img/facebook-icon-e6def5.png\n",
      "/img/email-icon-a4c505.png\n"
     ]
    }
   ],
   "source": [
    "for key, images in cleaned_dictionary.items():\n",
    "    for image in images:\n",
    "        print(image.attrs['src'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the scope of this tutorial, I have decided to:\n",
    "    \n",
    "- Remove the logo links with the //\n",
    "- Add on the domain to the relative URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = []\n",
    "\n",
    "for key, images in cleaned_dictionary.items():\n",
    "    # 1. Creating a clean_urls and domain name for every page:\n",
    "    clean_urls = []\n",
    "    domain_name = tldextract.extract(key).registered_domain\n",
    "    # 2. Looping over every image per url:\n",
    "    for image in images:\n",
    "        # 3. Extracting the source (src) with .attrs:\n",
    "        source_image_url = image.attrs['src']\n",
    "        # 4. Clean The Data\n",
    "        if source_image_url.startswith(\"//\"):\n",
    "            pass\n",
    "        elif domain_name not in source_image_url and 'http' not in source_image_url:\n",
    "            url = 'https://' + domain_name + source_image_url\n",
    "            all_images.append(url)\n",
    "        else:\n",
    "            all_images.append(source_image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://understandingdata.com/wp-content/uploads/2019/09/james-anthony-phoenix.jpg', 'https://understandingdata.com/wp-content/uploads/2019/09/james-anthony-phoenix.jpg', 'https://understandingdata.com/wp-content/uploads/2020/05/Is-Web-Scraping-Illegal-370x370.png', 'https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g', 'https://understandingdata.com/wp-content/uploads/2020/03/web-scraping-tools-370x192.jpg']\n"
     ]
    }
   ],
   "source": [
    "print(all_images[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After obtaining our list of clean image URLs we can now refer to method one for extracting the images to our computer! \n",
    "\n",
    "This time let's convert it into a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_images(image_urls_list:list, directory_path):\n",
    "    \n",
    "    # Changing directory into a specific folder:\n",
    "    os.chdir(directory_path)\n",
    "    \n",
    "    # Downloading all of the images\n",
    "    for img in image_urls_list:\n",
    "        file_name = img.split('/')[-1]\n",
    "        \n",
    "        # Let's try both of these versions in a loop [https:// and https://www.]\n",
    "        url_paths_to_try = [img, img.replace('https://', 'https://www.')]\n",
    "        for url_image_path in url_paths_to_try:\n",
    "            print(url_image_path)\n",
    "            try:\n",
    "                r = requests.get(img, stream=True)\n",
    "                if r.status_code == 200:\n",
    "                    with open(file_name, 'wb') as f:\n",
    "                        for chunk in r:\n",
    "                            f.write(chunk)\n",
    "            except Exception as e:\n",
    "                pass        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jamesaphoenix/Desktop/Imran_And_James/Python_For_SEO/6_downloading_multiple_images/all_images\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://understandingdata.com/wp-content/uploads/2019/09/james-anthony-phoenix.jpg\n",
      "https://www.understandingdata.com/wp-content/uploads/2019/09/james-anthony-phoenix.jpg\n",
      "https://understandingdata.com/wp-content/uploads/2019/09/james-anthony-phoenix.jpg\n",
      "https://www.understandingdata.com/wp-content/uploads/2019/09/james-anthony-phoenix.jpg\n",
      "https://understandingdata.com/wp-content/uploads/2020/05/Is-Web-Scraping-Illegal-370x370.png\n",
      "https://www.understandingdata.com/wp-content/uploads/2020/05/Is-Web-Scraping-Illegal-370x370.png\n",
      "https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://www.secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://understandingdata.com/wp-content/uploads/2020/03/web-scraping-tools-370x192.jpg\n",
      "https://www.understandingdata.com/wp-content/uploads/2020/03/web-scraping-tools-370x192.jpg\n",
      "https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://www.secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://understandingdata.com/wp-content/uploads/2020/03/community-detection-370x238.png\n",
      "https://www.understandingdata.com/wp-content/uploads/2020/03/community-detection-370x238.png\n",
      "https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://www.secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://understandingdata.com/wp-content/uploads/2020/03/what-is-web-scraping-370x370.png\n",
      "https://www.understandingdata.com/wp-content/uploads/2020/03/what-is-web-scraping-370x370.png\n",
      "https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://www.secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://understandingdata.com/wp-content/uploads/2020/03/installing-chromedriver-headless-browser.png\n",
      "https://www.understandingdata.com/wp-content/uploads/2020/03/installing-chromedriver-headless-browser.png\n",
      "https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://www.secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://understandingdata.com/wp-content/uploads/2020/02/how-to-install-python-anaconda-mac-370x208.png\n",
      "https://www.understandingdata.com/wp-content/uploads/2020/02/how-to-install-python-anaconda-mac-370x208.png\n",
      "https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://www.secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://understandingdata.com/wp-content/uploads/2019/12/ninjaoutreach-black-friday-deal-370x194.png\n",
      "https://www.understandingdata.com/wp-content/uploads/2019/12/ninjaoutreach-black-friday-deal-370x194.png\n",
      "https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://www.secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://understandingdata.com/wp-content/uploads/2020/02/webscraping_advantages_disadvantages-370x370.png\n",
      "https://www.understandingdata.com/wp-content/uploads/2020/02/webscraping_advantages_disadvantages-370x370.png\n",
      "https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://www.secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://understandingdata.com/wp-content/uploads/2019/11/prospecting-seo-clients-with-python-370x370.png\n",
      "https://www.understandingdata.com/wp-content/uploads/2019/11/prospecting-seo-clients-with-python-370x370.png\n",
      "https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://www.secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://www.understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://www.understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://www.understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://www.understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://www.understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://www.understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://www.understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://www.understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/uploads/2020/05/data-engineering-services.jpg\n",
      "https://www.understandingdata.com/wp-content/uploads/2020/05/data-engineering-services.jpg\n",
      "https://understandingdata.com/wp-content/uploads/2020/05/data_pipelines.png\n",
      "https://www.understandingdata.com/wp-content/uploads/2020/05/data_pipelines.png\n",
      "https://understandingdata.com/wp-content/uploads/2020/05/041-filter.png\n",
      "https://www.understandingdata.com/wp-content/uploads/2020/05/041-filter.png\n",
      "https://understandingdata.com/wp-content/uploads/2020/05/016-laptop-2.png\n",
      "https://www.understandingdata.com/wp-content/uploads/2020/05/016-laptop-2.png\n",
      "https://understandingdata.com/wp-content/uploads/2020/05/006-database-1.png\n",
      "https://www.understandingdata.com/wp-content/uploads/2020/05/006-database-1.png\n",
      "https://understandingdata.com/wp-content/uploads/2020/05/data_engineering_service.jpg\n",
      "https://www.understandingdata.com/wp-content/uploads/2020/05/data_engineering_service.jpg\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://www.understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://www.understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://www.understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://www.understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://www.understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://www.understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://www.understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://www.understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://internetingishard.com/img/interneting-is-hard-logo-97b225.svg\n",
      "https://www.internetingishard.com/img/interneting-is-hard-logo-97b225.svg\n",
      "https://internetingishard.com/html-and-css/links-and-images/links-and-images-6820c7.png\n",
      "https://www.internetingishard.com/html-and-css/links-and-images/links-and-images-6820c7.png\n",
      "https://internetingishard.com/html-and-css/links-and-images/links-and-images-example-7ab70f.png\n",
      "https://www.internetingishard.com/html-and-css/links-and-images/links-and-images-example-7ab70f.png\n",
      "https://internetingishard.com/html-and-css/links-and-images/example-files-b21613.png\n",
      "https://www.internetingishard.com/html-and-css/links-and-images/example-files-b21613.png\n",
      "https://internetingishard.com/html-and-css/links-and-images/html-attributes-6f5690.png\n",
      "https://www.internetingishard.com/html-and-css/links-and-images/html-attributes-6f5690.png\n",
      "https://internetingishard.com/html-and-css/links-and-images/html-link-href-element-61348e.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.internetingishard.com/html-and-css/links-and-images/html-link-href-element-61348e.png\n",
      "https://internetingishard.com/html-and-css/links-and-images/absolute-relative-root-relative-links-104560.png\n",
      "https://www.internetingishard.com/html-and-css/links-and-images/absolute-relative-root-relative-links-104560.png\n",
      "https://internetingishard.com/html-and-css/links-and-images/absolute-link-syntax-64d730.png\n",
      "https://www.internetingishard.com/html-and-css/links-and-images/absolute-link-syntax-64d730.png\n",
      "https://internetingishard.com/html-and-css/links-and-images/absolute-links-32f469.png\n",
      "https://www.internetingishard.com/html-and-css/links-and-images/absolute-links-32f469.png\n",
      "https://internetingishard.com/html-and-css/links-and-images/relative-links-e178d0.png\n",
      "https://www.internetingishard.com/html-and-css/links-and-images/relative-links-e178d0.png\n",
      "https://internetingishard.com/html-and-css/links-and-images/relative-link-no-parent-4629d0.png\n",
      "https://www.internetingishard.com/html-and-css/links-and-images/relative-link-no-parent-4629d0.png\n",
      "https://internetingishard.com/html-and-css/links-and-images/relative-link-with-parent-666b79.png\n",
      "https://www.internetingishard.com/html-and-css/links-and-images/relative-link-with-parent-666b79.png\n",
      "https://internetingishard.com/html-and-css/links-and-images/root-relative-links-368060.png\n",
      "https://www.internetingishard.com/html-and-css/links-and-images/root-relative-links-368060.png\n",
      "https://internetingishard.com/html-and-css/links-and-images/image-formats-62b23d.png\n",
      "https://www.internetingishard.com/html-and-css/links-and-images/image-formats-62b23d.png\n",
      "https://internetingishard.com/html-and-css/links-and-images/mochi-77c69d.jpg\n",
      "https://www.internetingishard.com/html-and-css/links-and-images/mochi-77c69d.jpg\n",
      "https://internetingishard.com/html-and-css/links-and-images/mochi-961ee5.gif\n",
      "https://www.internetingishard.com/html-and-css/links-and-images/mochi-961ee5.gif\n",
      "https://internetingishard.com/html-and-css/links-and-images/mochi-f95fdf.png\n",
      "https://www.internetingishard.com/html-and-css/links-and-images/mochi-f95fdf.png\n",
      "https://internetingishard.com/html-and-css/links-and-images/svg-vs-png-image-3df7fd.png\n",
      "https://www.internetingishard.com/html-and-css/links-and-images/svg-vs-png-image-3df7fd.png\n",
      "https://internetingishard.com/html-and-css/links-and-images/html-img-formats-3b43c6.png\n",
      "https://www.internetingishard.com/html-and-css/links-and-images/html-img-formats-3b43c6.png\n",
      "https://internetingishard.com/html-and-css/links-and-images/html-character-sets-no-utf-8-304820.png\n",
      "https://www.internetingishard.com/html-and-css/links-and-images/html-character-sets-no-utf-8-304820.png\n",
      "https://internetingishard.com/img/interneting-is-hard-logo-97b225.svg\n",
      "https://www.internetingishard.com/img/interneting-is-hard-logo-97b225.svg\n",
      "https://internetingishard.com/img/twitter-icon-29cd78.png\n",
      "https://www.internetingishard.com/img/twitter-icon-29cd78.png\n",
      "https://internetingishard.com/img/facebook-icon-e6def5.png\n",
      "https://www.internetingishard.com/img/facebook-icon-e6def5.png\n",
      "https://internetingishard.com/img/email-icon-a4c505.png\n",
      "https://www.internetingishard.com/img/email-icon-a4c505.png\n"
     ]
    }
   ],
   "source": [
    "path = '/Users/jamesaphoenix/Desktop/Imran_And_James/Python_For_SEO/6_downloading_multiple_images/all_images'\n",
    "\n",
    "extract_images(image_urls_list=all_images, \n",
    "               directory_path=path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantastic! \n",
    "\n",
    "Now there are some things that we didn't necessarily cover for which include:\n",
    "\n",
    "- http:// only image urls.\n",
    "- http://www. only image urls.\n",
    "\n",
    "But for the most part, you can hopefully now hopefully download images in bulk!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![how to download multiple images within python](https://sempioneer.com/wp-content/uploads/2020/06/all_images.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How To Speed Up Your Image Downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its important when working with 100's or 1000's of URLs to avoid using as synchronous approach to downloading images. An asynchronous approach means that we can download multiple web pages or multiple images in parallel.\n",
    "\n",
    "<strong> This means the overall execution time will be much quicker! </strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ThreadPoolExecutor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_single_image(img):\n",
    "    file_name = img.split('/')[-1]\n",
    "    \n",
    "    # Let's try both of these versions in a loop [https:// and https://www.]\n",
    "    url_paths_to_try = [img, img.replace('https://', 'https://www.')]\n",
    "    for url_image_path in url_paths_to_try:\n",
    "        try:\n",
    "            r = requests.get(img, stream=True)\n",
    "            if r.status_code == 200:\n",
    "                with open(file_name, 'wb') as f:\n",
    "                    for chunk in r:\n",
    "                        f.write(chunk)\n",
    "            return \"Completed\"\n",
    "        except Exception as e:\n",
    "            return \"Failed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ThreadPoolExecutor is one of python's built in I/O packages for creating an asynchronous behaviour via multiple threads. In order to utilise it, we will make sure that the function will only work on a single URL.\n",
    "\n",
    "Then we will pass the image URL list into multiple workers ;) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://understandingdata.com/wp-content/uploads/2019/09/james-anthony-phoenix.jpg',\n",
       " 'https://understandingdata.com/wp-content/uploads/2019/09/james-anthony-phoenix.jpg',\n",
       " 'https://understandingdata.com/wp-content/uploads/2020/05/Is-Web-Scraping-Illegal-370x370.png',\n",
       " 'https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g',\n",
       " 'https://understandingdata.com/wp-content/uploads/2020/03/web-scraping-tools-370x192.jpg']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_images[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jamesaphoenix/Desktop/Imran_And_James/Python_For_SEO/6_downloading_multiple_images/all_images\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting the old image directory and creating a new one in its place: (FIX THIS )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/jamesaphoenix/Desktop/Imran_And_James/Python_For_SEO/6_downloading_multiple_images/all_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import urllib.request\n",
    "\n",
    "# We can use a with statement to ensure threads are cleaned up promptly\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    # Start the load operations and mark each future with its URL\n",
    "    future_to_url = {executor.submit(extract_single_image, image_url) for image_url in all_images}\n",
    "    for future in concurrent.futures.as_completed(future_to_url):\n",
    "        try:\n",
    "            url = future_to_url[future]\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        try:\n",
    "            data = future.result()\n",
    "        except Exception as exc:\n",
    "            print('%r generated an exception: %s' % (url, exc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should've downloaded the images but at a much faster rate! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Async Programming! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like JavaScript, Python 3.6+ comes bundled with native support for co-routines called [asyncio](https://docs.python.org/3/library/asyncio.html). Similar to NodeJS, there is a method available to you for creating custom event loops for async code. \n",
    "\n",
    "We will also need to download an async code HTTP requests library called [aiohttp](https://docs.aiohttp.org/en/stable/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.7/site-packages (3.6.2)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.7/site-packages (from aiohttp) (19.3.0)\r\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /opt/anaconda3/lib/python3.7/site-packages (from aiohttp) (3.0.1)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.7/site-packages (from aiohttp) (1.4.2)\r\n",
      "Requirement already satisfied: chardet<4.0,>=2.0 in /opt/anaconda3/lib/python3.7/site-packages (from aiohttp) (3.0.4)\r\n",
      "Requirement already satisfied: multidict<5.0,>=4.5 in /opt/anaconda3/lib/python3.7/site-packages (from aiohttp) (4.7.6)\r\n",
      "Requirement already satisfied: idna>=2.0 in /opt/anaconda3/lib/python3.7/site-packages (from yarl<2.0,>=1.0->aiohttp) (2.8)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install aiohttp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally as we are running this code within a Jupyter Notebook, which is actually inside of an event loop, \n",
    "we will need to install and apply [nest-asyncio](https://pypi.org/project/nest-asyncio/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nest-asyncio in /opt/anaconda3/lib/python3.7/site-packages (1.3.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install nest-asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pro-tip: Whenever you re-factor your async code from a Jupyter notebook, you will never need to use nest-asyncio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only Use This In Jupyter Notebooks!\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to structure our code slightly different for the async version to work:\n",
    "    \n",
    "1. We will have a fetch function to query the webpage.\n",
    "2. We will have a parse function to get all of the image URLs per webpage.\n",
    "3. We will have an extract function to download all of the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch(session, url):\n",
    "    try:\n",
    "        async with session.get(url) as response:\n",
    "            # Notice how both of the functions are await in our async def fetch function!\n",
    "            content = await response.text()\n",
    "            image_urls = await parse(content)\n",
    "            return images\n",
    "    except Exception as e:\n",
    "        print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def parse(text):\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    images = soup.findAll('img')\n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch_single_image(domain, image, session):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def extract(images:list):\n",
    "    for image in images:\n",
    "        await fetch_single_image(image, session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main(web_pages):\n",
    "    all_data = []\n",
    "    tasks = []\n",
    "    \n",
    "    headers = {\n",
    "        \"user-agent\": \"Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)\"}\n",
    "    async with aiohttp.ClientSession(headers=headers) as session:\n",
    "        for page in web_pages:\n",
    "            tasks.append(fetch(session, url))\n",
    "        htmls = await asyncio.gather(*tasks)\n",
    "        all_data.extend(htmls)\n",
    "        \n",
    "    print(len(all_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_urls = []\n",
    "for key, value in cleaned_dictionary.items():\n",
    "    all_urls.extend(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n"
     ]
    }
   ],
   "source": [
    "print(len(all_urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: The object should be created from async function\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n"
     ]
    }
   ],
   "source": [
    "loop = asyncio.new_event_loop()\n",
    "asyncio.set_event_loop(loop)\n",
    "loop.run_until_complete(main(all_urls))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
