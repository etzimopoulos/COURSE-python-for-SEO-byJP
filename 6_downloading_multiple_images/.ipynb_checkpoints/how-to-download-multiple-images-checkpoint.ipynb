{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. How To Download Multiple Images In Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Outcomes\n",
    "\n",
    "- To learn how to download multiple images in Python using synchronous and asynchronous code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatically downloading images from a number of your HTML pages is an essential skill, in this guide you'll be learning 4 methods on how to download images using Python! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin with the easiest example, if you already have a list of image URLs then we can follow this process:\n",
    "\n",
    "1. Change into a directory where we would like to store all of the images.\n",
    "2. Make a request to download all of the images, one by one.\n",
    "3. We will also include error handling so that if a URL no longer exists the code will still work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tldextract in /opt/anaconda3/lib/python3.7/site-packages (2.2.2)\n",
      "Requirement already satisfied: requests>=2.1.0 in /opt/anaconda3/lib/python3.7/site-packages (from tldextract) (2.22.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.7/site-packages (from tldextract) (46.0.0.post20200309)\n",
      "Requirement already satisfied: requests-file>=1.4 in /opt/anaconda3/lib/python3.7/site-packages (from tldextract) (1.5.1)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.7/site-packages (from tldextract) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda3/lib/python3.7/site-packages (from requests>=2.1.0->tldextract) (1.25.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.7/site-packages (from requests>=2.1.0->tldextract) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.7/site-packages (from requests>=2.1.0->tldextract) (2019.11.28)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.7/site-packages (from requests-file>=1.4->tldextract) (1.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tldextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import subprocess\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import tldextract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir all_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mall_images\u001b[m\u001b[m                            how-to-download-multiple-images.ipynb\r\n",
      "asyncio-aiofiles.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing into the directory of the folder called all_images, this can be done by either:\n",
    "\n",
    "~~~\n",
    "\n",
    "cd all_images\n",
    "os.chdir('path')\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('all_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jamesaphoenix/Desktop/Imran_And_James/Python_For_SEO/6_downloading_multiple_images/all_images\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method One: How To Download Multiple Images From A Python List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to download the multiple images, we can use the [requests library](https://requests.readthedocs.io/en/master/). We'll also create a python list to store any images that didn't have a 200 status code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "broken_images = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_urls = ['https://sempioneer.com/wp-content/uploads/2020/05/dataframe-300x84.png',\n",
    "             'https://sempioneer.com/wp-content/uploads/2020/05/json_format_data-300x72.png']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the file name: dataframe-300x84.png\n",
      "This is the file name: json_format_data-300x72.png\n"
     ]
    }
   ],
   "source": [
    "for img in image_urls:\n",
    "    # We can split the file based upon / and extract the last split within the python list below:\n",
    "    file_name = img.split('/')[-1]\n",
    "    print(f\"This is the file name: {file_name}\")\n",
    "    # Now let's send a request to the image URL:\n",
    "    r = requests.get(img, stream=True)\n",
    "    # We can check that the status code is 200 before doing anything else:\n",
    "    if r.status_code == 200:\n",
    "        # This command below will allow us to write the data to a file as binary:\n",
    "        with open(file_name, 'wb') as f:\n",
    "            for chunk in r:\n",
    "                f.write(chunk)\n",
    "    else:\n",
    "        # We will write all of the images back to the broken_images list:\n",
    "        broken_images.append(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "☝️ See how simple that is! ☝️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you check your folder, you will have now downloaded all of the images that contained a status code of 200! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![downloading images correctly with python](https://sempioneer.com/wp-content/uploads/2020/06/how-to-download-images-with-python.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method Two: How To Download Multiple Images From Many HTML Web Pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we don't yet have the exact image URLs, we will need to do the following:\n",
    "\n",
    "1. Download the HTML content of every web page.\n",
    "2. Extract all of the image URLs for every page.\n",
    "3. Create the file names.\n",
    "4. Check to see if the image status code is 200.\n",
    "5. Write all of images to your local computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This website [internetingishard.com](https://www.internetingishard.com/html-and-css/links-and-images/) has some relative image URLs. Therefore we will need to ensure that our code can handle for the following two types of image source URLs:\n",
    "\n",
    "---\n",
    "\n",
    "- <strong> Exact Filepath: https://www.internetingishard.com/html-and-css/links-and-images/html-attributes-6f5690.png </strong>\n",
    "- <strong> Relative Filepath: /html-and-css/links-and-images/html-attributes-6f5690.png </strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_pages = ['https://understandingdata.com/', \n",
    "             'https://understandingdata.com/data-engineering-services/',\n",
    "             'https://www.internetingishard.com/html-and-css/links-and-images/']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also extract the domain of every URL whilst we loop over the webpages like so:\n",
    "    \n",
    "~~~\n",
    "\n",
    "for page in webpages:\n",
    "    domain_name = tldextract.extract(page).registered_domain\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_dictionary = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The domain name: understandingdata.com\n",
      "The domain name: understandingdata.com\n",
      "The domain name: internetingishard.com\n"
     ]
    }
   ],
   "source": [
    "for page in web_pages:\n",
    "    # 1. Extracting the domain name of the web page:\n",
    "    domain_name = tldextract.extract(page).registered_domain\n",
    "    print(f\"The domain name: {domain_name}\")    \n",
    "    # 2. Request the web page:\n",
    "    r = requests.get(page)\n",
    "    # 3. Check to see if the web page returned a status_200:\n",
    "    if r.status_code == 200:\n",
    "        \n",
    "        # 4. Create a URL dictionary entry for future use:\n",
    "        url_dictionary[page] = []\n",
    "        \n",
    "        # 5. Parse the HTML content with BeautifulSoup and look for image tags:\n",
    "        soup = BeautifulSoup(r.content, 'html.parser')\n",
    "        \n",
    "        # 6. Find all of the images per web page:\n",
    "        images = soup.findAll('img')\n",
    "        \n",
    "        # 7. Store all of the images \n",
    "        url_dictionary[page].extend(images)\n",
    "        \n",
    "    else:\n",
    "        print('failed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's double check and filter our dictionary so that we only look at web pages where there was at least 1 image tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This domain: https://understandingdata.com/ has more than 1 image on the web page.\n",
      "This domain: https://understandingdata.com/data-engineering-services/ has more than 1 image on the web page.\n",
      "This domain: https://www.internetingishard.com/html-and-css/links-and-images/ has more than 1 image on the web page.\n"
     ]
    }
   ],
   "source": [
    "for key, value in url_dictionary.items():\n",
    "    if len(value) > 0:\n",
    "        print(f\"This domain: {key} has more than 1 image on the web page.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An easier way to write the above code would be via a dictionary comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dictionary = {key: value for key, value in url_dictionary.items() if len(value) > 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now clean all of the image URLs inside of every dictionary key and change all of the relative URL paths to exact URL paths.\n",
    "\n",
    "Let's start by printing out all of the different image sources to see how we might need to clean up the data below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "//understandingdata.com/wp-content/uploads/2019/04/cropped-logo_transparent-1.png\n",
      "//understandingdata.com/wp-content/uploads/2019/04/cropped-logo_transparent-1.png\n",
      "https://understandingdata.com/wp-content/uploads/2019/09/james-anthony-phoenix.jpg\n",
      "https://understandingdata.com/wp-content/uploads/2019/09/james-anthony-phoenix.jpg\n",
      "https://understandingdata.com/wp-content/uploads/2020/05/Is-Web-Scraping-Illegal-370x370.png\n",
      "https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://understandingdata.com/wp-content/uploads/2020/03/web-scraping-tools-370x192.jpg\n",
      "https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://understandingdata.com/wp-content/uploads/2020/03/community-detection-370x238.png\n",
      "https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://understandingdata.com/wp-content/uploads/2020/03/what-is-web-scraping-370x370.png\n",
      "https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://understandingdata.com/wp-content/uploads/2020/03/installing-chromedriver-headless-browser.png\n",
      "https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://understandingdata.com/wp-content/uploads/2020/02/how-to-install-python-anaconda-mac-370x208.png\n",
      "https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://understandingdata.com/wp-content/uploads/2019/12/ninjaoutreach-black-friday-deal-370x194.png\n",
      "https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://understandingdata.com/wp-content/uploads/2020/02/webscraping_advantages_disadvantages-370x370.png\n",
      "https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://understandingdata.com/wp-content/uploads/2019/11/prospecting-seo-clients-with-python-370x370.png\n",
      "https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "//understandingdata.com/wp-content/uploads/2019/04/cropped-logo_transparent-1.png\n",
      "//understandingdata.com/wp-content/uploads/2019/04/cropped-logo_transparent-1.png\n",
      "https://understandingdata.com/wp-content/uploads/2020/05/data-engineering-services.jpg\n",
      "https://understandingdata.com/wp-content/uploads/2020/05/data_pipelines.png\n",
      "https://understandingdata.com/wp-content/uploads/2020/05/041-filter.png\n",
      "https://understandingdata.com/wp-content/uploads/2020/05/016-laptop-2.png\n",
      "https://understandingdata.com/wp-content/uploads/2020/05/006-database-1.png\n",
      "https://understandingdata.com/wp-content/uploads/2020/05/data_engineering_service.jpg\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "/img/interneting-is-hard-logo-97b225.svg\n",
      "/html-and-css/links-and-images/links-and-images-6820c7.png\n",
      "/html-and-css/links-and-images/links-and-images-example-7ab70f.png\n",
      "/html-and-css/links-and-images/example-files-b21613.png\n",
      "/html-and-css/links-and-images/html-attributes-6f5690.png\n",
      "/html-and-css/links-and-images/html-link-href-element-61348e.png\n",
      "/html-and-css/links-and-images/absolute-relative-root-relative-links-104560.png\n",
      "/html-and-css/links-and-images/absolute-link-syntax-64d730.png\n",
      "/html-and-css/links-and-images/absolute-links-32f469.png\n",
      "/html-and-css/links-and-images/relative-links-e178d0.png\n",
      "/html-and-css/links-and-images/relative-link-no-parent-4629d0.png\n",
      "/html-and-css/links-and-images/relative-link-with-parent-666b79.png\n",
      "/html-and-css/links-and-images/root-relative-links-368060.png\n",
      "/html-and-css/links-and-images/image-formats-62b23d.png\n",
      "/html-and-css/links-and-images/mochi-77c69d.jpg\n",
      "/html-and-css/links-and-images/mochi-961ee5.gif\n",
      "/html-and-css/links-and-images/mochi-f95fdf.png\n",
      "/html-and-css/links-and-images/svg-vs-png-image-3df7fd.png\n",
      "/html-and-css/links-and-images/html-img-formats-3b43c6.png\n",
      "/html-and-css/links-and-images/html-character-sets-no-utf-8-304820.png\n",
      "/img/interneting-is-hard-logo-97b225.svg\n",
      "/img/twitter-icon-29cd78.png\n",
      "/img/facebook-icon-e6def5.png\n",
      "/img/email-icon-a4c505.png\n"
     ]
    }
   ],
   "source": [
    "for key, images in cleaned_dictionary.items():\n",
    "    for image in images:\n",
    "        print(image.attrs['src'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the scope of this tutorial, I have decided to:\n",
    "    \n",
    "- Remove the logo links with the //\n",
    "- Add on the domain to the relative URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = []\n",
    "\n",
    "for key, images in cleaned_dictionary.items():\n",
    "    # 1. Creating a clean_urls and domain name for every page:\n",
    "    clean_urls = []\n",
    "    domain_name = tldextract.extract(key).registered_domain\n",
    "    # 2. Looping over every image per url:\n",
    "    for image in images:\n",
    "        # 3. Extracting the source (src) with .attrs:\n",
    "        source_image_url = image.attrs['src']\n",
    "        # 4. Clean The Data\n",
    "        if source_image_url.startswith(\"//\"):\n",
    "            pass\n",
    "        elif domain_name not in source_image_url and 'http' not in source_image_url:\n",
    "            url = 'https://' + domain_name + source_image_url\n",
    "            all_images.append(url)\n",
    "        else:\n",
    "            all_images.append(source_image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://understandingdata.com/wp-content/uploads/2019/09/james-anthony-phoenix.jpg', 'https://understandingdata.com/wp-content/uploads/2019/09/james-anthony-phoenix.jpg', 'https://understandingdata.com/wp-content/uploads/2020/05/Is-Web-Scraping-Illegal-370x370.png', 'https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g', 'https://understandingdata.com/wp-content/uploads/2020/03/web-scraping-tools-370x192.jpg']\n"
     ]
    }
   ],
   "source": [
    "print(all_images[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cleaning the image URLs, we can now refer to method one for downloading the images to our computer! \n",
    "\n",
    "This time let's convert it into a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_images(image_urls_list:list, directory_path):\n",
    "    \n",
    "    # Changing directory into a specific folder:\n",
    "    os.chdir(directory_path)\n",
    "    \n",
    "    # Downloading all of the images\n",
    "    for img in image_urls_list:\n",
    "        file_name = img.split('/')[-1]\n",
    "        \n",
    "        # Let's try both of these versions in a loop [https:// and https://www.]\n",
    "        url_paths_to_try = [img, img.replace('https://', 'https://www.')]\n",
    "        for url_image_path in url_paths_to_try:\n",
    "            print(url_image_path)\n",
    "            try:\n",
    "                r = requests.get(img, stream=True)\n",
    "                if r.status_code == 200:\n",
    "                    with open(file_name, 'wb') as f:\n",
    "                        for chunk in r:\n",
    "                            f.write(chunk)\n",
    "            except Exception as e:\n",
    "                pass        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jamesaphoenix/Desktop/Imran_And_James/Python_For_SEO/6_downloading_multiple_images/all_images\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://understandingdata.com/wp-content/uploads/2019/09/james-anthony-phoenix.jpg\n",
      "https://www.understandingdata.com/wp-content/uploads/2019/09/james-anthony-phoenix.jpg\n",
      "https://understandingdata.com/wp-content/uploads/2019/09/james-anthony-phoenix.jpg\n",
      "https://www.understandingdata.com/wp-content/uploads/2019/09/james-anthony-phoenix.jpg\n",
      "https://understandingdata.com/wp-content/uploads/2020/05/Is-Web-Scraping-Illegal-370x370.png\n",
      "https://www.understandingdata.com/wp-content/uploads/2020/05/Is-Web-Scraping-Illegal-370x370.png\n",
      "https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://www.secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://understandingdata.com/wp-content/uploads/2020/03/web-scraping-tools-370x192.jpg\n",
      "https://www.understandingdata.com/wp-content/uploads/2020/03/web-scraping-tools-370x192.jpg\n",
      "https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://www.secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://understandingdata.com/wp-content/uploads/2020/03/community-detection-370x238.png\n",
      "https://www.understandingdata.com/wp-content/uploads/2020/03/community-detection-370x238.png\n",
      "https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://www.secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://understandingdata.com/wp-content/uploads/2020/03/what-is-web-scraping-370x370.png\n",
      "https://www.understandingdata.com/wp-content/uploads/2020/03/what-is-web-scraping-370x370.png\n",
      "https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://www.secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://understandingdata.com/wp-content/uploads/2020/03/installing-chromedriver-headless-browser.png\n",
      "https://www.understandingdata.com/wp-content/uploads/2020/03/installing-chromedriver-headless-browser.png\n",
      "https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://www.secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://understandingdata.com/wp-content/uploads/2020/02/how-to-install-python-anaconda-mac-370x208.png\n",
      "https://www.understandingdata.com/wp-content/uploads/2020/02/how-to-install-python-anaconda-mac-370x208.png\n",
      "https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://www.secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://understandingdata.com/wp-content/uploads/2019/12/ninjaoutreach-black-friday-deal-370x194.png\n",
      "https://www.understandingdata.com/wp-content/uploads/2019/12/ninjaoutreach-black-friday-deal-370x194.png\n",
      "https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://www.secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://understandingdata.com/wp-content/uploads/2020/02/webscraping_advantages_disadvantages-370x370.png\n",
      "https://www.understandingdata.com/wp-content/uploads/2020/02/webscraping_advantages_disadvantages-370x370.png\n",
      "https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://www.secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://understandingdata.com/wp-content/uploads/2019/11/prospecting-seo-clients-with-python-370x370.png\n",
      "https://www.understandingdata.com/wp-content/uploads/2019/11/prospecting-seo-clients-with-python-370x370.png\n",
      "https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://www.secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://www.understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://www.understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://www.understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://www.understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://www.understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://www.understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://www.understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://www.understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/uploads/2020/05/data-engineering-services.jpg\n",
      "https://www.understandingdata.com/wp-content/uploads/2020/05/data-engineering-services.jpg\n",
      "https://understandingdata.com/wp-content/uploads/2020/05/data_pipelines.png\n",
      "https://www.understandingdata.com/wp-content/uploads/2020/05/data_pipelines.png\n",
      "https://understandingdata.com/wp-content/uploads/2020/05/041-filter.png\n",
      "https://www.understandingdata.com/wp-content/uploads/2020/05/041-filter.png\n",
      "https://understandingdata.com/wp-content/uploads/2020/05/016-laptop-2.png\n",
      "https://www.understandingdata.com/wp-content/uploads/2020/05/016-laptop-2.png\n",
      "https://understandingdata.com/wp-content/uploads/2020/05/006-database-1.png\n",
      "https://www.understandingdata.com/wp-content/uploads/2020/05/006-database-1.png\n",
      "https://understandingdata.com/wp-content/uploads/2020/05/data_engineering_service.jpg\n",
      "https://www.understandingdata.com/wp-content/uploads/2020/05/data_engineering_service.jpg\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://www.understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://www.understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://www.understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://www.understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://www.understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://www.understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://www.understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://www.understandingdata.com/wp-content/plugins/instagram-feed/img/placeholder.png\n",
      "https://internetingishard.com/img/interneting-is-hard-logo-97b225.svg\n",
      "https://www.internetingishard.com/img/interneting-is-hard-logo-97b225.svg\n",
      "https://internetingishard.com/html-and-css/links-and-images/links-and-images-6820c7.png\n",
      "https://www.internetingishard.com/html-and-css/links-and-images/links-and-images-6820c7.png\n",
      "https://internetingishard.com/html-and-css/links-and-images/links-and-images-example-7ab70f.png\n",
      "https://www.internetingishard.com/html-and-css/links-and-images/links-and-images-example-7ab70f.png\n",
      "https://internetingishard.com/html-and-css/links-and-images/example-files-b21613.png\n",
      "https://www.internetingishard.com/html-and-css/links-and-images/example-files-b21613.png\n",
      "https://internetingishard.com/html-and-css/links-and-images/html-attributes-6f5690.png\n",
      "https://www.internetingishard.com/html-and-css/links-and-images/html-attributes-6f5690.png\n",
      "https://internetingishard.com/html-and-css/links-and-images/html-link-href-element-61348e.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.internetingishard.com/html-and-css/links-and-images/html-link-href-element-61348e.png\n",
      "https://internetingishard.com/html-and-css/links-and-images/absolute-relative-root-relative-links-104560.png\n",
      "https://www.internetingishard.com/html-and-css/links-and-images/absolute-relative-root-relative-links-104560.png\n",
      "https://internetingishard.com/html-and-css/links-and-images/absolute-link-syntax-64d730.png\n",
      "https://www.internetingishard.com/html-and-css/links-and-images/absolute-link-syntax-64d730.png\n",
      "https://internetingishard.com/html-and-css/links-and-images/absolute-links-32f469.png\n",
      "https://www.internetingishard.com/html-and-css/links-and-images/absolute-links-32f469.png\n",
      "https://internetingishard.com/html-and-css/links-and-images/relative-links-e178d0.png\n",
      "https://www.internetingishard.com/html-and-css/links-and-images/relative-links-e178d0.png\n",
      "https://internetingishard.com/html-and-css/links-and-images/relative-link-no-parent-4629d0.png\n",
      "https://www.internetingishard.com/html-and-css/links-and-images/relative-link-no-parent-4629d0.png\n",
      "https://internetingishard.com/html-and-css/links-and-images/relative-link-with-parent-666b79.png\n",
      "https://www.internetingishard.com/html-and-css/links-and-images/relative-link-with-parent-666b79.png\n",
      "https://internetingishard.com/html-and-css/links-and-images/root-relative-links-368060.png\n",
      "https://www.internetingishard.com/html-and-css/links-and-images/root-relative-links-368060.png\n",
      "https://internetingishard.com/html-and-css/links-and-images/image-formats-62b23d.png\n",
      "https://www.internetingishard.com/html-and-css/links-and-images/image-formats-62b23d.png\n",
      "https://internetingishard.com/html-and-css/links-and-images/mochi-77c69d.jpg\n",
      "https://www.internetingishard.com/html-and-css/links-and-images/mochi-77c69d.jpg\n",
      "https://internetingishard.com/html-and-css/links-and-images/mochi-961ee5.gif\n",
      "https://www.internetingishard.com/html-and-css/links-and-images/mochi-961ee5.gif\n",
      "https://internetingishard.com/html-and-css/links-and-images/mochi-f95fdf.png\n",
      "https://www.internetingishard.com/html-and-css/links-and-images/mochi-f95fdf.png\n",
      "https://internetingishard.com/html-and-css/links-and-images/svg-vs-png-image-3df7fd.png\n",
      "https://www.internetingishard.com/html-and-css/links-and-images/svg-vs-png-image-3df7fd.png\n",
      "https://internetingishard.com/html-and-css/links-and-images/html-img-formats-3b43c6.png\n",
      "https://www.internetingishard.com/html-and-css/links-and-images/html-img-formats-3b43c6.png\n",
      "https://internetingishard.com/html-and-css/links-and-images/html-character-sets-no-utf-8-304820.png\n",
      "https://www.internetingishard.com/html-and-css/links-and-images/html-character-sets-no-utf-8-304820.png\n",
      "https://internetingishard.com/img/interneting-is-hard-logo-97b225.svg\n",
      "https://www.internetingishard.com/img/interneting-is-hard-logo-97b225.svg\n",
      "https://internetingishard.com/img/twitter-icon-29cd78.png\n",
      "https://www.internetingishard.com/img/twitter-icon-29cd78.png\n",
      "https://internetingishard.com/img/facebook-icon-e6def5.png\n",
      "https://www.internetingishard.com/img/facebook-icon-e6def5.png\n",
      "https://internetingishard.com/img/email-icon-a4c505.png\n",
      "https://www.internetingishard.com/img/email-icon-a4c505.png\n"
     ]
    }
   ],
   "source": [
    "path = '/Users/jamesaphoenix/Desktop/Imran_And_James/Python_For_SEO/6_downloading_multiple_images/all_images'\n",
    "\n",
    "extract_images(image_urls_list=all_images, \n",
    "               directory_path=path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantastic! \n",
    "\n",
    "Now there are some things that we didn't necessarily cover for which include:\n",
    "\n",
    "- http:// only image urls.\n",
    "- http://www. only image urls.\n",
    "\n",
    "But for the most part, you'll be able to download images in bulk!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![how to download multiple images within python](https://sempioneer.com/wp-content/uploads/2020/06/all_images.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How To Speed Up Your Image Downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with 100's or 1000's of URLs its important to avoid using a synchronous approach to downloading images.  An asynchronous approach means that we can download multiple web pages or multiple images in parallel.\n",
    "\n",
    "<strong> This means that the overall execution time will be much quicker! </strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ThreadPoolExecutor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ThreadPoolExecutor is one of python's built in I/O packages for creating an asynchronous behaviour via multiple threads. In order to utilise it, we will make sure that the function will only work on a single URL.\n",
    "\n",
    "Then we will pass the image URL list into multiple workers ;) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_single_image(img):\n",
    "    file_name = img.split('/')[-1]\n",
    "    \n",
    "    # Let's try both of these versions in a loop [https:// and https://www.]\n",
    "    url_paths_to_try = [img, img.replace('https://', 'https://www.')]\n",
    "    for url_image_path in url_paths_to_try:\n",
    "        try:\n",
    "            r = requests.get(img, stream=True)\n",
    "            if r.status_code == 200:\n",
    "                with open(file_name, 'wb') as f:\n",
    "                    for chunk in r:\n",
    "                        f.write(chunk)\n",
    "            return \"Completed\"\n",
    "        except Exception as e:\n",
    "            return \"Failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://understandingdata.com/wp-content/uploads/2019/09/james-anthony-phoenix.jpg',\n",
       " 'https://understandingdata.com/wp-content/uploads/2019/09/james-anthony-phoenix.jpg',\n",
       " 'https://understandingdata.com/wp-content/uploads/2020/05/Is-Web-Scraping-Illegal-370x370.png',\n",
       " 'https://secure.gravatar.com/avatar/17d8a69424a54d3957e1ce51755c6cfd?s=35&r=g',\n",
       " 'https://understandingdata.com/wp-content/uploads/2020/03/web-scraping-tools-370x192.jpg']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_images[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code will create a new directory and then make it the current active working directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir('/Users/jamesaphoenix/Desktop/Imran_And_James/Python_For_SEO/6_downloading_multiple_images/all_images_asnyc')\n",
    "\n",
    "except FileExistsError as e:\n",
    "    print('The file path already exists!')\n",
    "\n",
    "os.chdir('/Users/jamesaphoenix/Desktop/Imran_And_James/Python_For_SEO/6_downloading_multiple_images/all_images_asnyc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import urllib.request\n",
    "\n",
    "# We can use a with statement to ensure threads are cleaned up promptly\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    # Start the load operations and mark each future with its URL\n",
    "    future_to_url = {executor.submit(extract_single_image, image_url) for image_url in all_images}\n",
    "    for future in concurrent.futures.as_completed(future_to_url):\n",
    "        try:\n",
    "            url = future_to_url[future]\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        try:\n",
    "            data = future.result()\n",
    "        except Exception as exc:\n",
    "            print('%r generated an exception: %s' % (url, exc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should've downloaded the images but at a much faster rate! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Async Programming! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like JavaScript, Python 3.6+ comes bundled with native support for co-routines called [asyncio](https://docs.python.org/3/library/asyncio.html). Similar to NodeJS, there is a method available to you for creating custom event loops for async code. \n",
    "\n",
    "We will also need to download an async code HTTP requests library called [aiohttp](https://docs.aiohttp.org/en/stable/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.7/site-packages (3.6.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.7/site-packages (from aiohttp) (1.4.2)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /opt/anaconda3/lib/python3.7/site-packages (from aiohttp) (3.0.1)\n",
      "Requirement already satisfied: multidict<5.0,>=4.5 in /opt/anaconda3/lib/python3.7/site-packages (from aiohttp) (4.7.6)\n",
      "Requirement already satisfied: chardet<4.0,>=2.0 in /opt/anaconda3/lib/python3.7/site-packages (from aiohttp) (3.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.7/site-packages (from aiohttp) (19.3.0)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/anaconda3/lib/python3.7/site-packages (from yarl<2.0,>=1.0->aiohttp) (2.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install aiohttp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also download aiofiles that allows us to write multiple image files asynchronously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aiofiles in /opt/anaconda3/lib/python3.7/site-packages (0.5.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install aiofiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import aiofiles\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir('/Users/jamesaphoenix/Desktop/Imran_And_James/Python_For_SEO/6_downloading_multiple_images/all_images_asnyc_event_loop')\n",
    "except FileExistsError as e:\n",
    "    print('The file path already exists!')\n",
    "\n",
    "os.chdir('/Users/jamesaphoenix/Desktop/Imran_And_James/Python_For_SEO/6_downloading_multiple_images/all_images_asnyc_event_loop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jamesaphoenix/Desktop/Imran_And_James/Python_For_SEO/6_downloading_multiple_images/all_images_asnyc_event_loop\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How To Download 1 File Asychronously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://understandingdata.com/wp-content/uploads/2019/09/james-anthony-phoenix.jpg']\n"
     ]
    }
   ],
   "source": [
    "print(all_images[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_image = 'https://understandingdata.com/wp-content/uploads/2019/09/james-anthony-phoenix.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "async with aiohttp.ClientSession() as session:\n",
    "    async with session.get(single_image) as resp:\n",
    "        # 1. Capturing the image file name like we did before:\n",
    "        single_image_name = single_image.split('/')[-1]\n",
    "        # 2. Only proceed further if the HTTP response is 200 (Ok)\n",
    "        if resp.status == 200:\n",
    "            async with aiofiles.open(single_image_name, mode='wb') as f:\n",
    "                await f.write(await resp.read())\n",
    "                await f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Downloading one image with aiofiles](https://sempioneer.com/wp-content/uploads/2020/06/image_files.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We will need to structure our code slightly different for the async version to work across multiple file:\n",
    "\n",
    "1. We will have a fetch function to query every image URL.\n",
    "2. We will have a main function that creates, then executes a series of co-routines.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch(session, url):\n",
    "    async with session.get(url) as resp:\n",
    "        # 1. Capturing the image file name like we did before:\n",
    "        url_name = url.split('/')[-1]\n",
    "        # 2. Only proceed further if the HTTP response is 200 (Ok)\n",
    "        if resp.status == 200:\n",
    "            async with aiofiles.open(url_name, mode='wb') as f:\n",
    "                await f.write(await resp.read())\n",
    "                await f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main(image_urls:list):\n",
    "    tasks = []\n",
    "    headers = {\n",
    "        \"user-agent\": \"Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)\"}\n",
    "    async with aiohttp.ClientSession(headers=headers) as session:\n",
    "        for image in image_urls:\n",
    "            tasks.append(await fetch(session, url))\n",
    "    data = await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object main at 0x107746e60>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main(all_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "☝️☝️☝️ Notice how when we call this function, it doesn't actually run and produces a [co-routine!](https://docs.python.org/3/library/asyncio-task.html) ☝️☝️☝️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use asyncio as method for executing all of the fetch callables that need to be completed:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Error with asyncio.run](https://sempioneer.com/wp-content/uploads/2020/06/error-downloading-python-files.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you receive this type of error when running the following command:\n",
    "\n",
    "~~~\n",
    "\n",
    "asyncio.run(main(all_images))\n",
    "\n",
    "~~~\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<strong> It is likely because you're trying to run asyncio within an event loop which is not natively possible. (Jupyter notebook runs in an event loop!). </strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How To Download Multiple Python Files Inside Of A Python File (.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the variable containing our URLs to a .txt file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('images.txt', 'w') as f:\n",
    "    for item in all_images:\n",
    "        f.write(f\"{item}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create A Python File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you will need to create a python file and add the following code to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package / Module Imports\n",
    "import aiohttp\n",
    "import aiofiles\n",
    "import asyncio\n",
    "import os\n",
    "\n",
    "# 1. Choose A Path - You will need to change this to your desired directory:\n",
    "path = '/Users/jamesaphoenix/Desktop/Imran_And_James/Python_For_SEO/6_downloading_multiple_images/all_images_asnyc_event_loop'\n",
    "\n",
    "try:\n",
    "    os.mkdir('/Users/jamesaphoenix/Desktop/Imran_And_James/Python_For_SEO/6_downloading_multiple_images/all_images_asnyc_event_loop')\n",
    "except FileExistsError as e:\n",
    "    print('The file path already exists!')\n",
    "\n",
    "# 2. Changing directory into that specific path:\n",
    "os.chdir('/Users/jamesaphoenix/Desktop/Imran_And_James/Python_For_SEO/6_downloading_multiple_images/all_images_asnyc_event_loop')\n",
    "\n",
    "\n",
    "# 3. Reading the URLs from the text file:\n",
    "with open('images.txt', 'r') as f:\n",
    "    image_urls = f.read().split('\\n')\n",
    "\n",
    "# 2. Creating the async functions:\n",
    "async def fetch(session, url):\n",
    "    async with session.get(url) as resp:\n",
    "        # 1. Capturing the image file name like we did before:\n",
    "        url_name = url.split('/')[-1]\n",
    "        # 2. Only proceed further if the HTTP response is 200 (Ok)\n",
    "        if resp.status == 200:\n",
    "            async with aiofiles.open(url_name, mode='wb') as f:\n",
    "                await f.write(await resp.read())\n",
    "                await f.close()\n",
    "\n",
    "async def main(image_urls:list):\n",
    "    tasks = []\n",
    "    headers = {\n",
    "        \"user-agent\": \"Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)\"}\n",
    "    async with aiohttp.ClientSession(headers=headers) as session:\n",
    "        for image in image_urls:\n",
    "            tasks.append(await fetch(session, image))\n",
    "    data = await asyncio.gather(*tasks)\n",
    "\n",
    "\n",
    "# 3. Executing all of the asyncio tasks:\n",
    "try:\n",
    "    asyncio.run(main(image_urls))\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then run the python script in <strong> either your terminal / command line with: </strong>\n",
    "    \n",
    "    \n",
    "~~~\n",
    "\n",
    "python3 python_file_name.py\n",
    "\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down what's happening in the above code snippet:\n",
    "    \n",
    "1. We are importing all of the relevant packages for async programming with files.\n",
    "2. Then we create a new directory.\n",
    "3. After creating the new folder we change that folder to be the active working directory.\n",
    "4. We then read the variable data which was previously saved from the file called images.txt\n",
    "5. Then we create a series of co-routines and execute them within a main() function with asyncio.\n",
    "6. As these co-routines are executed every file is asynchronously saved to your computer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![downloading multiple files with asyncio-aiohttp](https://sempioneer.com/wp-content/uploads/2020/06/asyncio-with-aiofiles.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's clear up and delete all of the folders to clean up our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders_to_delete = [\n",
    "    \n",
    "    '/Users/jamesaphoenix/Desktop/Imran_And_James/Python_For_SEO/6_downloading_multiple_images/all_images_asnyc_event_loop',\n",
    "    '/Users/jamesaphoenix/Desktop/Imran_And_James/Python_For_SEO/6_downloading_multiple_images/all_images',\n",
    "    '/Users/jamesaphoenix/Desktop/Imran_And_James/Python_For_SEO/6_downloading_multiple_images/all_images_asnyc'\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for folder in folders_to_delete:\n",
    "        print(f\"Deleting this folder directory: {folder}\")\n",
    "        print('------')\n",
    "        shutil.rmtree(folder)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether you decide to download images synchronously or asynchronously, its important to realise that although you can do this in tools such as ScreamingFrog or with Google Chrome Extensions. Being able to download images with python allows you to extend your automation capabilities and what other programs, APIs etc you might use that image data with! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
